{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
   },
   "source": [
    "### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.\n",
    "### If any part is not clear, please comment.  \n",
    "### Please upvote if it was helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwademo123/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
    "\n",
    "from scipy.signal import chirp, find_peaks, peak_widths\n",
    "import pywt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
   },
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        x = K.concatenate([weighted_input, x], axis=2)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], self.features_dim*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('../input/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
   },
   "outputs": [],
   "source": [
    "# in other notebook I have extracted the min and max values from the train data, the measurements\n",
    "max_num = 127\n",
    "min_num = -128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
   },
   "outputs": [],
   "source": [
    "# This function standardize the data from (-128 to 127) to (-1 to 1)\n",
    "# Theoretically it helps in the NN Model training, but I didn't tested without it\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddest(d, axis=None):\n",
    "    \"\"\"\n",
    "    Mean Absolute Deviation\n",
    "    \"\"\"\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal( x, wavelet='db4', level=1):\n",
    "    \"\"\"\n",
    "    1. Adapted from waveletSmooth function found here:\n",
    "    http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "    2. Threshold equation and using hard mode in threshold as mentioned\n",
    "    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "    http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\", level=level)\n",
    "    \n",
    "    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "\n",
    "    # Calculte the univeral threshold\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec( coeff[1:], wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal_2( x, wavelet='db4', level=1):\n",
    "    \"\"\"\n",
    "    1. Adapted from waveletSmooth function found here:\n",
    "    http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "    2. Threshold equation and using hard mode in threshold as mentioned\n",
    "    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "    http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\", level=level)\n",
    "    \n",
    "    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "\n",
    "    # Calculte the univeral threshold\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec( coeff[0:], wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corona(x_dn, maxDistance=10, maxHeightRatio=0.25, maxTicksRemoval=500):\n",
    "    index = pd.Series(x_dn).loc[np.abs(x_dn)>0].index\n",
    "    corona_idx = []\n",
    "    for idx in index:\n",
    "        for i in range(1,maxDistance+1):\n",
    "            if idx+i < pd.Series(x_dn).shape[0]:\n",
    "                if x_dn[idx+i]/(x_dn[idx]+1e-04)<-maxHeightRatio:\n",
    "                    x_dn[idx:idx+maxTicksRemoval] = 0\n",
    "                    corona_idx.append(idx)\n",
    "    return x_dn, corona_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    \n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        \n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "                \n",
    "        \n",
    "        \n",
    "        feat_array = np.asarray([mean, std, std_top, std_bot, max_range], dtype=np.float32)\n",
    "        \n",
    "        new_ts.append(np.concatenate([feat_array\n",
    "                                      , percentil_calc, relative_percentile]))\n",
    "    \n",
    "        \n",
    "    return new_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)                \n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(np.asarray(praq_train[str(signal_id)], dtype=np.float32)))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    \n",
    "    \n",
    "    roopsize = [2500, 400000]\n",
    "    \n",
    "    for r_size in roopsize:\n",
    "        roop = praq_train.iloc[praq_train.shape[0]-r_size:, :]\n",
    "        praq_train_2 = pd.concat([roop, praq_train], axis=0).iloc[:800000, :]\n",
    "    \n",
    "        for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):    \n",
    "            X_signal = []\n",
    "            # for each phase of the signal\n",
    "            for phase in [0,1,2]:\n",
    "                # extract from df_train both signal_id and target to compose the new data sets\n",
    "                signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "                # but just append the target one time, to not triplicate it\n",
    "                if phase == 0:\n",
    "                    y.append(target)                \n",
    "                # extract and transform data into sets of features\n",
    "                X_signal.append(transform_ts(np.asarray(praq_train_2[str(signal_id)], dtype=np.float32)))\n",
    "            # concatenate all the 3 phases in one matrix\n",
    "            X_signal = np.concatenate(X_signal, axis=1)\n",
    "            # add the data to X\n",
    "            X.append(X_signal)\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "       \n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtrain(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X, y = prep_data(start, end)\n",
    "    return idx, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [04:50<00:00,  1.31it/s]\n",
      "100%|██████████| 363/363 [04:50<00:00,  1.23it/s]\n",
      "100%|██████████| 363/363 [04:51<00:00,  1.17it/s]\n",
      "100%|██████████| 363/363 [04:51<00:00,  1.29it/s]\n",
      "  0%|          | 0/363 [00:00<?, ?it/s] 1.38it/s]\n",
      "100%|██████████| 363/363 [04:53<00:00,  1.39it/s]\n",
      "100%|██████████| 363/363 [04:53<00:00,  1.65it/s]\n",
      "100%|██████████| 363/363 [04:54<00:00,  1.37it/s]\n",
      "100%|██████████| 363/363 [04:51<00:00,  1.39it/s]\n",
      "100%|█████████▉| 362/363 [04:55<00:00,  1.26it/s]\n",
      "100%|██████████| 363/363 [04:54<00:00,  1.24it/s]\n",
      "100%|██████████| 363/363 [04:56<00:00,  1.38it/s]\n",
      "100%|██████████| 363/363 [04:53<00:00,  1.32it/s]\n",
      "100%|██████████| 363/363 [04:54<00:00,  1.42it/s]\n",
      "100%|██████████| 363/363 [04:56<00:00,  1.38it/s]\n",
      "100%|██████████| 363/363 [04:57<00:00,  1.44it/s]\n",
      "100%|██████████| 363/363 [04:34<00:00,  1.41it/s]\n",
      "100%|██████████| 363/363 [04:36<00:00,  1.49it/s]\n",
      "100%|██████████| 363/363 [04:38<00:00,  1.76it/s]\n",
      "100%|██████████| 363/363 [04:39<00:00,  1.70it/s]\n",
      "100%|██████████| 363/363 [04:38<00:00,  2.02it/s]\n",
      "100%|██████████| 363/363 [04:38<00:00,  1.82it/s]\n",
      "100%|██████████| 363/363 [04:39<00:00,  3.71it/s]\n",
      "100%|██████████| 363/363 [04:40<00:00,  4.36it/s]\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# this code is very simple, divide the total size of the df_train into two sets and process it\n",
    "#X = []\n",
    "#y = []\n",
    "all_chunks = []\n",
    "\n",
    "num_cores = 8 \n",
    "#def load_all():\n",
    "total_size = len(df_train)\n",
    "chunk_size = total_size/num_cores\n",
    "\n",
    "for i in range(8):\n",
    "    start_idx = int(i * chunk_size)\n",
    "    end_idx = int(start_idx + chunk_size)\n",
    "    chunk = (start_idx, end_idx, i)\n",
    "    all_chunks.append(chunk)\n",
    "\n",
    "pool = Pool()\n",
    "results = pool.map(process_subtrain, all_chunks)    \n",
    "results = sorted(results, key=lambda tup: tup[0])\n",
    "\n",
    "X = np.concatenate([item[1] for item in results], axis=0)\n",
    "y = np.concatenate([item[2] for item in results], axis=0)\n",
    "\n",
    "#load_all()\n",
    "\n",
    "#X = np.asarray(X)\n",
    "#y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8712, 160, 57) (8712,)\n"
     ]
    }
   ],
   "source": [
    "# The X shape here is very important. It is also important undertand a little how a LSTM works\n",
    "# X.shape[0] is the number of id_measuremts contained in train data\n",
    "# X.shape[1] is the number of chunks resultant of the transformation, each of this date enters in the LSTM serialized\n",
    "# This way the LSTM can understand the position of a data relative with other and activate a signal that needs\n",
    "# a serie of inputs in a specifc order.\n",
    "# X.shape[3] is the number of features multiplied by the number of phases (3)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 int32\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
   },
   "outputs": [],
   "source": [
    "# save data into file, a numpy specific format\n",
    "np.save(\"X_9.npy\",X)\n",
    "np.save(\"y_9.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./X.npy\")\n",
    "y = np.load(\"./y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(Layer):\n",
    "  \n",
    "    def __init__(self, depth:int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.depth = depth\n",
    "        self.q_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.k_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.v_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.output_dense_layer = Dense(depth, use_bias=False)\n",
    "    \n",
    "    def call(self, inp):\n",
    "        q = self.q_dense_layer(inp)  # [batch_size, q_length, depth]\n",
    "        q *= self.depth ** -0.5\n",
    "        print(q.shape)\n",
    "        \n",
    "        k = self.k_dense_layer(inp)  # [batch_size, m_length, depth]\n",
    "        v = self.v_dense_layer(inp)\n",
    "\n",
    "        logit = tf.matmul(q, k, transpose_b=True)\n",
    "        print(logit.shape)\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(logit, name='attention_weight')\n",
    "        \n",
    "        attention_output = tf.matmul(attention_weight, v)  # [batch_size, q_length, depth]\n",
    "        print(attention_output.shape)\n",
    "        \n",
    "        x = self.output_dense_layer(attention_output) + q\n",
    "        print(x.shape)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(input_shape)==3:\n",
    "            return input_shape[0], input_shape[1], self.depth\n",
    "        if len(input_shape)==4:\n",
    "            return input_shape[0], input_shape[1], input_shape[2], self.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2]))\n",
    "    \n",
    "    x = Bidirectional(LSTM(30, return_sequences=True))(inp)\n",
    "    x = Bidirectional(LSTM(30, return_sequences=True))(x)\n",
    "    \n",
    "    x = Attention(input_shape[1])(x)\n",
    "    x = Lambda(lambda x: K.sum(x, axis=1))(x)\n",
    "\n",
    "\n",
    "    x = Activation('tanh')(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 6969 samples, validate on 1743 samples\n",
      "Epoch 1/50\n",
      "6969/6969 [==============================] - 21s 3ms/step - loss: 0.1966 - matthews_correlation: 0.0066 - val_loss: 0.1548 - val_matthews_correlation: 0.0356\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.03562, saving model to weights_0.h5\n",
      "Epoch 2/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1416 - matthews_correlation: 0.0162 - val_loss: 0.1278 - val_matthews_correlation: 0.2698\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.03562 to 0.26980, saving model to weights_0.h5\n",
      "Epoch 3/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.1159 - matthews_correlation: 0.4764 - val_loss: 0.1100 - val_matthews_correlation: 0.5490\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.26980 to 0.54904, saving model to weights_0.h5\n",
      "Epoch 4/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.1060 - matthews_correlation: 0.6108 - val_loss: 0.1017 - val_matthews_correlation: 0.6468\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.54904 to 0.64681, saving model to weights_0.h5\n",
      "Epoch 5/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.1036 - matthews_correlation: 0.6166 - val_loss: 0.1025 - val_matthews_correlation: 0.5752\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.64681\n",
      "Epoch 6/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.1027 - matthews_correlation: 0.5895 - val_loss: 0.1005 - val_matthews_correlation: 0.6538\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.64681 to 0.65378, saving model to weights_0.h5\n",
      "Epoch 7/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0983 - matthews_correlation: 0.6492 - val_loss: 0.0975 - val_matthews_correlation: 0.6478\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 8/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0975 - matthews_correlation: 0.6554 - val_loss: 0.1000 - val_matthews_correlation: 0.6450\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 9/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0988 - matthews_correlation: 0.6455 - val_loss: 0.1018 - val_matthews_correlation: 0.5917\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 10/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0950 - matthews_correlation: 0.6369 - val_loss: 0.1335 - val_matthews_correlation: 0.6144\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 11/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1020 - matthews_correlation: 0.6355 - val_loss: 0.0991 - val_matthews_correlation: 0.5938\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 12/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0945 - matthews_correlation: 0.6447 - val_loss: 0.0905 - val_matthews_correlation: 0.6467\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 13/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0935 - matthews_correlation: 0.6822 - val_loss: 0.0912 - val_matthews_correlation: 0.6403\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.65378\n",
      "Epoch 14/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0920 - matthews_correlation: 0.6468 - val_loss: 0.0892 - val_matthews_correlation: 0.6670\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.65378 to 0.66702, saving model to weights_0.h5\n",
      "Epoch 15/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0948 - matthews_correlation: 0.6705 - val_loss: 0.0931 - val_matthews_correlation: 0.6668\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 16/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0946 - matthews_correlation: 0.6506 - val_loss: 0.0967 - val_matthews_correlation: 0.5935\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 17/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0940 - matthews_correlation: 0.6664 - val_loss: 0.1069 - val_matthews_correlation: 0.6632\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 18/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1024 - matthews_correlation: 0.6600 - val_loss: 0.0930 - val_matthews_correlation: 0.6394\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 19/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0899 - matthews_correlation: 0.6831 - val_loss: 0.1020 - val_matthews_correlation: 0.5655\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 20/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0901 - matthews_correlation: 0.6839 - val_loss: 0.0941 - val_matthews_correlation: 0.6422\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 21/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0876 - matthews_correlation: 0.7117 - val_loss: 0.0929 - val_matthews_correlation: 0.6339\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 22/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0893 - matthews_correlation: 0.6883 - val_loss: 0.0886 - val_matthews_correlation: 0.6501\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 23/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0895 - matthews_correlation: 0.6903 - val_loss: 0.0875 - val_matthews_correlation: 0.6613\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 24/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0871 - matthews_correlation: 0.6937 - val_loss: 0.0875 - val_matthews_correlation: 0.6427\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 25/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0873 - matthews_correlation: 0.7097 - val_loss: 0.1023 - val_matthews_correlation: 0.5469\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 26/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0872 - matthews_correlation: 0.6910 - val_loss: 0.0959 - val_matthews_correlation: 0.5976\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 27/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0905 - matthews_correlation: 0.6600 - val_loss: 0.0902 - val_matthews_correlation: 0.6455\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 28/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0866 - matthews_correlation: 0.6939 - val_loss: 0.0887 - val_matthews_correlation: 0.6434\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.66702\n",
      "Epoch 29/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0857 - matthews_correlation: 0.7176 - val_loss: 0.0940 - val_matthews_correlation: 0.6981\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.66702 to 0.69811, saving model to weights_0.h5\n",
      "Epoch 30/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0886 - matthews_correlation: 0.7132 - val_loss: 0.0883 - val_matthews_correlation: 0.6912\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 31/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0857 - matthews_correlation: 0.7140 - val_loss: 0.0974 - val_matthews_correlation: 0.6653\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 32/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0830 - matthews_correlation: 0.7218 - val_loss: 0.0875 - val_matthews_correlation: 0.6704\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 33/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0916 - matthews_correlation: 0.6676 - val_loss: 0.1127 - val_matthews_correlation: 0.6129\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0951 - matthews_correlation: 0.6672 - val_loss: 0.0890 - val_matthews_correlation: 0.6830\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 35/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0855 - matthews_correlation: 0.7098 - val_loss: 0.0890 - val_matthews_correlation: 0.6245\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 36/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0854 - matthews_correlation: 0.7177 - val_loss: 0.0898 - val_matthews_correlation: 0.6374\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 37/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0834 - matthews_correlation: 0.7122 - val_loss: 0.0880 - val_matthews_correlation: 0.6840\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 38/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0837 - matthews_correlation: 0.7310 - val_loss: 0.0835 - val_matthews_correlation: 0.6893\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 39/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0823 - matthews_correlation: 0.7260 - val_loss: 0.0835 - val_matthews_correlation: 0.6932\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.69811\n",
      "Epoch 40/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0803 - matthews_correlation: 0.7308 - val_loss: 0.0838 - val_matthews_correlation: 0.7053\n",
      "\n",
      "Epoch 00040: val_matthews_correlation improved from 0.69811 to 0.70534, saving model to weights_0.h5\n",
      "Epoch 41/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0803 - matthews_correlation: 0.7247 - val_loss: 0.0814 - val_matthews_correlation: 0.6789\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.70534\n",
      "Epoch 42/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0820 - matthews_correlation: 0.7347 - val_loss: 0.0868 - val_matthews_correlation: 0.6742\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.70534\n",
      "Epoch 43/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0873 - matthews_correlation: 0.6928 - val_loss: 0.0850 - val_matthews_correlation: 0.6350\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.70534\n",
      "Epoch 44/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0873 - matthews_correlation: 0.6859 - val_loss: 0.0834 - val_matthews_correlation: 0.6536\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.70534\n",
      "Epoch 45/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0811 - matthews_correlation: 0.7512 - val_loss: 0.0805 - val_matthews_correlation: 0.7093\n",
      "\n",
      "Epoch 00045: val_matthews_correlation improved from 0.70534 to 0.70935, saving model to weights_0.h5\n",
      "Epoch 46/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0795 - matthews_correlation: 0.7360 - val_loss: 0.0811 - val_matthews_correlation: 0.6743\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.70935\n",
      "Epoch 47/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0802 - matthews_correlation: 0.7459 - val_loss: 0.0824 - val_matthews_correlation: 0.6720\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.70935\n",
      "Epoch 48/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0786 - matthews_correlation: 0.7250 - val_loss: 0.0825 - val_matthews_correlation: 0.7230\n",
      "\n",
      "Epoch 00048: val_matthews_correlation improved from 0.70935 to 0.72296, saving model to weights_0.h5\n",
      "Epoch 49/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0790 - matthews_correlation: 0.7293 - val_loss: 0.0825 - val_matthews_correlation: 0.6834\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.72296\n",
      "Epoch 50/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0780 - matthews_correlation: 0.7406 - val_loss: 0.0809 - val_matthews_correlation: 0.7269\n",
      "\n",
      "Epoch 00050: val_matthews_correlation improved from 0.72296 to 0.72686, saving model to weights_0.h5\n",
      "Beginning fold 2\n",
      "Train on 6969 samples, validate on 1743 samples\n",
      "Epoch 1/50\n",
      "6969/6969 [==============================] - 22s 3ms/step - loss: 0.2465 - matthews_correlation: 0.0032 - val_loss: 0.1914 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_1.h5\n",
      "Epoch 2/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1804 - matthews_correlation: 0.0000e+00 - val_loss: 0.1680 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1629 - matthews_correlation: 0.0000e+00 - val_loss: 0.1527 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1494 - matthews_correlation: 0.0000e+00 - val_loss: 0.1431 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 5/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1315 - matthews_correlation: 0.2056 - val_loss: 0.1181 - val_matthews_correlation: 0.2514\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.00000 to 0.25139, saving model to weights_1.h5\n",
      "Epoch 6/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1240 - matthews_correlation: 0.3282 - val_loss: 0.1260 - val_matthews_correlation: 0.6333\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.25139 to 0.63332, saving model to weights_1.h5\n",
      "Epoch 7/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1192 - matthews_correlation: 0.4944 - val_loss: 0.1104 - val_matthews_correlation: 0.5931\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.63332\n",
      "Epoch 8/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1120 - matthews_correlation: 0.5988 - val_loss: 0.1010 - val_matthews_correlation: 0.6769\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.63332 to 0.67688, saving model to weights_1.h5\n",
      "Epoch 9/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1029 - matthews_correlation: 0.6632 - val_loss: 0.0966 - val_matthews_correlation: 0.6719\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 10/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1026 - matthews_correlation: 0.6580 - val_loss: 0.0926 - val_matthews_correlation: 0.6691\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 11/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1003 - matthews_correlation: 0.6331 - val_loss: 0.0946 - val_matthews_correlation: 0.6271\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 12/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.1004 - matthews_correlation: 0.6213 - val_loss: 0.0959 - val_matthews_correlation: 0.6592\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 13/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0967 - matthews_correlation: 0.6347 - val_loss: 0.0919 - val_matthews_correlation: 0.6604\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 14/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0982 - matthews_correlation: 0.6577 - val_loss: 0.0924 - val_matthews_correlation: 0.6097\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 15/50\n",
      "6969/6969 [==============================] - 16s 2ms/step - loss: 0.0973 - matthews_correlation: 0.6550 - val_loss: 0.0926 - val_matthews_correlation: 0.6588\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.67688\n",
      "Epoch 16/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0937 - matthews_correlation: 0.6701 - val_loss: 0.0926 - val_matthews_correlation: 0.7024\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.67688 to 0.70238, saving model to weights_1.h5\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0954 - matthews_correlation: 0.6707 - val_loss: 0.0902 - val_matthews_correlation: 0.6547\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 18/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0944 - matthews_correlation: 0.6595 - val_loss: 0.0896 - val_matthews_correlation: 0.6390\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 19/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0937 - matthews_correlation: 0.6156 - val_loss: 0.0885 - val_matthews_correlation: 0.6880\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 20/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0964 - matthews_correlation: 0.6604 - val_loss: 0.0901 - val_matthews_correlation: 0.6483\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 21/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0935 - matthews_correlation: 0.6653 - val_loss: 0.0884 - val_matthews_correlation: 0.6904\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 22/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0936 - matthews_correlation: 0.6678 - val_loss: 0.0906 - val_matthews_correlation: 0.6583\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 23/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0958 - matthews_correlation: 0.6625 - val_loss: 0.0957 - val_matthews_correlation: 0.6559\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 24/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0981 - matthews_correlation: 0.6171 - val_loss: 0.0899 - val_matthews_correlation: 0.6952\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.70238\n",
      "Epoch 25/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0924 - matthews_correlation: 0.6861 - val_loss: 0.0864 - val_matthews_correlation: 0.7180\n",
      "\n",
      "Epoch 00025: val_matthews_correlation improved from 0.70238 to 0.71801, saving model to weights_1.h5\n",
      "Epoch 26/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0983 - matthews_correlation: 0.5987 - val_loss: 0.0921 - val_matthews_correlation: 0.6204\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 27/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0944 - matthews_correlation: 0.6536 - val_loss: 0.0901 - val_matthews_correlation: 0.6966\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 28/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0921 - matthews_correlation: 0.6763 - val_loss: 0.0865 - val_matthews_correlation: 0.6772\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 29/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0903 - matthews_correlation: 0.6736 - val_loss: 0.0871 - val_matthews_correlation: 0.6949\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 30/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0898 - matthews_correlation: 0.7006 - val_loss: 0.0888 - val_matthews_correlation: 0.6506\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 31/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0890 - matthews_correlation: 0.7030 - val_loss: 0.0898 - val_matthews_correlation: 0.6528\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 32/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0907 - matthews_correlation: 0.6685 - val_loss: 0.0868 - val_matthews_correlation: 0.6777\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 33/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0876 - matthews_correlation: 0.6753 - val_loss: 0.0843 - val_matthews_correlation: 0.6850\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 34/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0896 - matthews_correlation: 0.6741 - val_loss: 0.0845 - val_matthews_correlation: 0.7088\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.71801\n",
      "Epoch 35/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0852 - matthews_correlation: 0.7036 - val_loss: 0.0825 - val_matthews_correlation: 0.7257\n",
      "\n",
      "Epoch 00035: val_matthews_correlation improved from 0.71801 to 0.72573, saving model to weights_1.h5\n",
      "Epoch 36/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0862 - matthews_correlation: 0.6881 - val_loss: 0.0851 - val_matthews_correlation: 0.6752\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 37/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0856 - matthews_correlation: 0.6905 - val_loss: 0.0848 - val_matthews_correlation: 0.6717\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 38/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0872 - matthews_correlation: 0.7049 - val_loss: 0.1042 - val_matthews_correlation: 0.7014\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 39/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0983 - matthews_correlation: 0.6694 - val_loss: 0.0870 - val_matthews_correlation: 0.6865\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 40/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0896 - matthews_correlation: 0.6843 - val_loss: 0.0839 - val_matthews_correlation: 0.7092\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 41/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0882 - matthews_correlation: 0.6777 - val_loss: 0.0827 - val_matthews_correlation: 0.6871\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 42/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0871 - matthews_correlation: 0.6860 - val_loss: 0.0823 - val_matthews_correlation: 0.7161\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 43/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0913 - matthews_correlation: 0.6758 - val_loss: 0.0936 - val_matthews_correlation: 0.6489\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 44/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0864 - matthews_correlation: 0.6934 - val_loss: 0.0849 - val_matthews_correlation: 0.6813\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 45/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0842 - matthews_correlation: 0.7064 - val_loss: 0.0906 - val_matthews_correlation: 0.6879\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 46/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0939 - matthews_correlation: 0.6691 - val_loss: 0.0853 - val_matthews_correlation: 0.6791\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 47/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0874 - matthews_correlation: 0.6876 - val_loss: 0.0883 - val_matthews_correlation: 0.6577\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 48/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0873 - matthews_correlation: 0.6817 - val_loss: 0.0838 - val_matthews_correlation: 0.6640\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 49/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0824 - matthews_correlation: 0.6966 - val_loss: 0.0818 - val_matthews_correlation: 0.6568\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.72573\n",
      "Epoch 50/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0810 - matthews_correlation: 0.7085 - val_loss: 0.0807 - val_matthews_correlation: 0.7249\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.72573\n",
      "Beginning fold 3\n",
      "Train on 6969 samples, validate on 1743 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969/6969 [==============================] - 21s 3ms/step - loss: 0.2153 - matthews_correlation: 0.0032 - val_loss: 0.1751 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_2.h5\n",
      "Epoch 2/50\n",
      "6969/6969 [==============================] - 17s 3ms/step - loss: 0.1604 - matthews_correlation: 0.0000e+00 - val_loss: 0.1519 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1329 - matthews_correlation: 0.0607 - val_loss: 0.1252 - val_matthews_correlation: 0.2375\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.23751, saving model to weights_2.h5\n",
      "Epoch 4/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1175 - matthews_correlation: 0.3686 - val_loss: 0.1133 - val_matthews_correlation: 0.6468\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.23751 to 0.64680, saving model to weights_2.h5\n",
      "Epoch 5/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1082 - matthews_correlation: 0.6031 - val_loss: 0.1058 - val_matthews_correlation: 0.6103\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.64680\n",
      "Epoch 6/50\n",
      "6969/6969 [==============================] - 18s 3ms/step - loss: 0.1034 - matthews_correlation: 0.6480 - val_loss: 0.1031 - val_matthews_correlation: 0.6315\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.64680\n",
      "Epoch 7/50\n",
      "6969/6969 [==============================] - 18s 3ms/step - loss: 0.1004 - matthews_correlation: 0.6365 - val_loss: 0.1009 - val_matthews_correlation: 0.6401\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.64680\n",
      "Epoch 8/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.1114 - matthews_correlation: 0.5664 - val_loss: 0.1066 - val_matthews_correlation: 0.6432\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.64680\n",
      "Epoch 9/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0983 - matthews_correlation: 0.6711 - val_loss: 0.0986 - val_matthews_correlation: 0.6873\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.64680 to 0.68734, saving model to weights_2.h5\n",
      "Epoch 10/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0967 - matthews_correlation: 0.6727 - val_loss: 0.0978 - val_matthews_correlation: 0.7167\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.68734 to 0.71667, saving model to weights_2.h5\n",
      "Epoch 11/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0959 - matthews_correlation: 0.6606 - val_loss: 0.1107 - val_matthews_correlation: 0.6008\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 12/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0977 - matthews_correlation: 0.6331 - val_loss: 0.1013 - val_matthews_correlation: 0.6579\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 13/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0948 - matthews_correlation: 0.6574 - val_loss: 0.1036 - val_matthews_correlation: 0.6005\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 14/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0930 - matthews_correlation: 0.6767 - val_loss: 0.0979 - val_matthews_correlation: 0.6721\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 15/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0916 - matthews_correlation: 0.6576 - val_loss: 0.0940 - val_matthews_correlation: 0.6672\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 16/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0924 - matthews_correlation: 0.6683 - val_loss: 0.0961 - val_matthews_correlation: 0.6534\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 17/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0917 - matthews_correlation: 0.6754 - val_loss: 0.0990 - val_matthews_correlation: 0.6325\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 18/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0911 - matthews_correlation: 0.6819 - val_loss: 0.0966 - val_matthews_correlation: 0.6896\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 19/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0902 - matthews_correlation: 0.6908 - val_loss: 0.0965 - val_matthews_correlation: 0.6800\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 20/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0887 - matthews_correlation: 0.6823 - val_loss: 0.0956 - val_matthews_correlation: 0.6451\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 21/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0907 - matthews_correlation: 0.6698 - val_loss: 0.0932 - val_matthews_correlation: 0.6857\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 22/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0892 - matthews_correlation: 0.6837 - val_loss: 0.1068 - val_matthews_correlation: 0.6404\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 23/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0961 - matthews_correlation: 0.6672 - val_loss: 0.0929 - val_matthews_correlation: 0.6481\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 24/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0904 - matthews_correlation: 0.6708 - val_loss: 0.0928 - val_matthews_correlation: 0.6832\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 25/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0897 - matthews_correlation: 0.7077 - val_loss: 0.0919 - val_matthews_correlation: 0.6911\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 26/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0870 - matthews_correlation: 0.6972 - val_loss: 0.0902 - val_matthews_correlation: 0.6726\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 27/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0875 - matthews_correlation: 0.6983 - val_loss: 0.0918 - val_matthews_correlation: 0.7049\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 28/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0853 - matthews_correlation: 0.7090 - val_loss: 0.0914 - val_matthews_correlation: 0.6824\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 29/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0856 - matthews_correlation: 0.6800 - val_loss: 0.0976 - val_matthews_correlation: 0.5952\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 30/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0909 - matthews_correlation: 0.6576 - val_loss: 0.0889 - val_matthews_correlation: 0.6483\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 31/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0851 - matthews_correlation: 0.6889 - val_loss: 0.0868 - val_matthews_correlation: 0.6911\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 32/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0845 - matthews_correlation: 0.7036 - val_loss: 0.0910 - val_matthews_correlation: 0.6575\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 33/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0895 - matthews_correlation: 0.7034 - val_loss: 0.0964 - val_matthews_correlation: 0.6815\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 34/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0871 - matthews_correlation: 0.7002 - val_loss: 0.0933 - val_matthews_correlation: 0.6653\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0859 - matthews_correlation: 0.6799 - val_loss: 0.0911 - val_matthews_correlation: 0.7047\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 36/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0850 - matthews_correlation: 0.7099 - val_loss: 0.0919 - val_matthews_correlation: 0.6404\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 37/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0846 - matthews_correlation: 0.7059 - val_loss: 0.0971 - val_matthews_correlation: 0.6449\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 38/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0874 - matthews_correlation: 0.6825 - val_loss: 0.0927 - val_matthews_correlation: 0.6593\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 39/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0861 - matthews_correlation: 0.6749 - val_loss: 0.0912 - val_matthews_correlation: 0.6933\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 40/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0842 - matthews_correlation: 0.7035 - val_loss: 0.0940 - val_matthews_correlation: 0.5990\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 41/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0871 - matthews_correlation: 0.6852 - val_loss: 0.0911 - val_matthews_correlation: 0.6740\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 42/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0818 - matthews_correlation: 0.7156 - val_loss: 0.0895 - val_matthews_correlation: 0.7112\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 43/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0800 - matthews_correlation: 0.7280 - val_loss: 0.0859 - val_matthews_correlation: 0.6929\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 44/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0806 - matthews_correlation: 0.7205 - val_loss: 0.0878 - val_matthews_correlation: 0.6963\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 45/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0862 - matthews_correlation: 0.7159 - val_loss: 0.0853 - val_matthews_correlation: 0.6932\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 46/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0899 - matthews_correlation: 0.7035 - val_loss: 0.0969 - val_matthews_correlation: 0.6840\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 47/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0816 - matthews_correlation: 0.7049 - val_loss: 0.0871 - val_matthews_correlation: 0.6933\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 48/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0802 - matthews_correlation: 0.7138 - val_loss: 0.0899 - val_matthews_correlation: 0.6451\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 49/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0807 - matthews_correlation: 0.7188 - val_loss: 0.0826 - val_matthews_correlation: 0.6954\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.71667\n",
      "Epoch 50/50\n",
      "6969/6969 [==============================] - 17s 2ms/step - loss: 0.0760 - matthews_correlation: 0.7503 - val_loss: 0.0919 - val_matthews_correlation: 0.6681\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.71667\n",
      "Beginning fold 4\n",
      "Train on 6970 samples, validate on 1742 samples\n",
      "Epoch 1/50\n",
      "6970/6970 [==============================] - 21s 3ms/step - loss: 0.2308 - matthews_correlation: -4.1763e-04 - val_loss: 0.1767 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_3.h5\n",
      "Epoch 2/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.1553 - matthews_correlation: 0.0000e+00 - val_loss: 0.1448 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.1255 - matthews_correlation: 0.0621 - val_loss: 0.1263 - val_matthews_correlation: 0.3651\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.36507, saving model to weights_3.h5\n",
      "Epoch 4/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.1114 - matthews_correlation: 0.5499 - val_loss: 0.1182 - val_matthews_correlation: 0.6486\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.36507 to 0.64863, saving model to weights_3.h5\n",
      "Epoch 5/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.1055 - matthews_correlation: 0.6295 - val_loss: 0.1194 - val_matthews_correlation: 0.5526\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.64863\n",
      "Epoch 6/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.1046 - matthews_correlation: 0.6164 - val_loss: 0.1177 - val_matthews_correlation: 0.6533\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.64863 to 0.65327, saving model to weights_3.h5\n",
      "Epoch 7/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0994 - matthews_correlation: 0.6515 - val_loss: 0.1136 - val_matthews_correlation: 0.6554\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.65327 to 0.65537, saving model to weights_3.h5\n",
      "Epoch 8/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0951 - matthews_correlation: 0.6576 - val_loss: 0.1229 - val_matthews_correlation: 0.5568\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.65537\n",
      "Epoch 9/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0937 - matthews_correlation: 0.6724 - val_loss: 0.1129 - val_matthews_correlation: 0.6656\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.65537 to 0.66559, saving model to weights_3.h5\n",
      "Epoch 10/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0926 - matthews_correlation: 0.6482 - val_loss: 0.1135 - val_matthews_correlation: 0.5695\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.66559\n",
      "Epoch 11/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0946 - matthews_correlation: 0.6592 - val_loss: 0.1238 - val_matthews_correlation: 0.5368\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.66559\n",
      "Epoch 12/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0965 - matthews_correlation: 0.6585 - val_loss: 0.1084 - val_matthews_correlation: 0.6848\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.66559 to 0.68478, saving model to weights_3.h5\n",
      "Epoch 13/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0904 - matthews_correlation: 0.6622 - val_loss: 0.1104 - val_matthews_correlation: 0.6312\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 14/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0905 - matthews_correlation: 0.6704 - val_loss: 0.1230 - val_matthews_correlation: 0.5178\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 15/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0973 - matthews_correlation: 0.6525 - val_loss: 0.1124 - val_matthews_correlation: 0.5694\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 16/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0904 - matthews_correlation: 0.6429 - val_loss: 0.1079 - val_matthews_correlation: 0.6417\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 17/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0919 - matthews_correlation: 0.6427 - val_loss: 0.1069 - val_matthews_correlation: 0.6454\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0907 - matthews_correlation: 0.6365 - val_loss: 0.1139 - val_matthews_correlation: 0.6435\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 19/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0937 - matthews_correlation: 0.6491 - val_loss: 0.1242 - val_matthews_correlation: 0.6687\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 20/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0884 - matthews_correlation: 0.6733 - val_loss: 0.1164 - val_matthews_correlation: 0.6822\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 21/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0908 - matthews_correlation: 0.6904 - val_loss: 0.1082 - val_matthews_correlation: 0.6700\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 22/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0873 - matthews_correlation: 0.6905 - val_loss: 0.1066 - val_matthews_correlation: 0.6589\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.68478\n",
      "Epoch 23/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0859 - matthews_correlation: 0.7010 - val_loss: 0.1119 - val_matthews_correlation: 0.6921\n",
      "\n",
      "Epoch 00023: val_matthews_correlation improved from 0.68478 to 0.69212, saving model to weights_3.h5\n",
      "Epoch 24/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0907 - matthews_correlation: 0.6661 - val_loss: 0.1043 - val_matthews_correlation: 0.7107\n",
      "\n",
      "Epoch 00024: val_matthews_correlation improved from 0.69212 to 0.71072, saving model to weights_3.h5\n",
      "Epoch 25/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0882 - matthews_correlation: 0.6844 - val_loss: 0.1160 - val_matthews_correlation: 0.6915\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 26/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0928 - matthews_correlation: 0.6780 - val_loss: 0.1113 - val_matthews_correlation: 0.6177\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 27/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0839 - matthews_correlation: 0.6686 - val_loss: 0.1042 - val_matthews_correlation: 0.6647\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 28/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0842 - matthews_correlation: 0.6891 - val_loss: 0.1049 - val_matthews_correlation: 0.6648\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 29/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0828 - matthews_correlation: 0.6891 - val_loss: 0.1053 - val_matthews_correlation: 0.6791\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 30/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0839 - matthews_correlation: 0.7156 - val_loss: 0.1112 - val_matthews_correlation: 0.6929\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 31/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0893 - matthews_correlation: 0.6893 - val_loss: 0.1077 - val_matthews_correlation: 0.6321\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 32/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0842 - matthews_correlation: 0.6947 - val_loss: 0.1066 - val_matthews_correlation: 0.6216\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.71072\n",
      "Epoch 33/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0845 - matthews_correlation: 0.6900 - val_loss: 0.1036 - val_matthews_correlation: 0.7363\n",
      "\n",
      "Epoch 00033: val_matthews_correlation improved from 0.71072 to 0.73629, saving model to weights_3.h5\n",
      "Epoch 34/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0805 - matthews_correlation: 0.6990 - val_loss: 0.1031 - val_matthews_correlation: 0.7221\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 35/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0801 - matthews_correlation: 0.6994 - val_loss: 0.1035 - val_matthews_correlation: 0.6766\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 36/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0784 - matthews_correlation: 0.7241 - val_loss: 0.1013 - val_matthews_correlation: 0.6605\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 37/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0798 - matthews_correlation: 0.7052 - val_loss: 0.1036 - val_matthews_correlation: 0.7203\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 38/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0792 - matthews_correlation: 0.7217 - val_loss: 0.1077 - val_matthews_correlation: 0.7082\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 39/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0816 - matthews_correlation: 0.7029 - val_loss: 0.1016 - val_matthews_correlation: 0.7082\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.73629\n",
      "Epoch 40/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0758 - matthews_correlation: 0.7272 - val_loss: 0.0983 - val_matthews_correlation: 0.7419\n",
      "\n",
      "Epoch 00040: val_matthews_correlation improved from 0.73629 to 0.74192, saving model to weights_3.h5\n",
      "Epoch 41/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0752 - matthews_correlation: 0.7339 - val_loss: 0.1012 - val_matthews_correlation: 0.7237\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.74192\n",
      "Epoch 42/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0858 - matthews_correlation: 0.6961 - val_loss: 0.1130 - val_matthews_correlation: 0.7113\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.74192\n",
      "Epoch 43/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0811 - matthews_correlation: 0.7017 - val_loss: 0.0998 - val_matthews_correlation: 0.7159\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.74192\n",
      "Epoch 44/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0767 - matthews_correlation: 0.7130 - val_loss: 0.1000 - val_matthews_correlation: 0.6880\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.74192\n",
      "Epoch 45/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0777 - matthews_correlation: 0.6927 - val_loss: 0.1083 - val_matthews_correlation: 0.6594\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.74192\n",
      "Epoch 46/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0775 - matthews_correlation: 0.7095 - val_loss: 0.0978 - val_matthews_correlation: 0.7499\n",
      "\n",
      "Epoch 00046: val_matthews_correlation improved from 0.74192 to 0.74988, saving model to weights_3.h5\n",
      "Epoch 47/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0750 - matthews_correlation: 0.7341 - val_loss: 0.0978 - val_matthews_correlation: 0.7255\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.74988\n",
      "Epoch 48/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0720 - matthews_correlation: 0.7547 - val_loss: 0.1105 - val_matthews_correlation: 0.6861\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.74988\n",
      "Epoch 49/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0808 - matthews_correlation: 0.7043 - val_loss: 0.1050 - val_matthews_correlation: 0.6657\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.74988\n",
      "Epoch 50/50\n",
      "6970/6970 [==============================] - 17s 2ms/step - loss: 0.0787 - matthews_correlation: 0.7056 - val_loss: 0.0987 - val_matthews_correlation: 0.7114\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.74988\n",
      "Beginning fold 5\n",
      "Train on 6971 samples, validate on 1741 samples\n",
      "Epoch 1/50\n",
      "6971/6971 [==============================] - 21s 3ms/step - loss: 0.2430 - matthews_correlation: -0.0015 - val_loss: 0.1978 - val_matthews_correlation: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_4.h5\n",
      "Epoch 2/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1881 - matthews_correlation: 0.0000e+00 - val_loss: 0.1682 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1645 - matthews_correlation: 0.0000e+00 - val_loss: 0.1454 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1379 - matthews_correlation: 0.1763 - val_loss: 0.1142 - val_matthews_correlation: 0.2998\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.00000 to 0.29983, saving model to weights_4.h5\n",
      "Epoch 5/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1195 - matthews_correlation: 0.4264 - val_loss: 0.1024 - val_matthews_correlation: 0.7120\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.29983 to 0.71199, saving model to weights_4.h5\n",
      "Epoch 6/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1096 - matthews_correlation: 0.6442 - val_loss: 0.0906 - val_matthews_correlation: 0.6766\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.71199\n",
      "Epoch 7/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1045 - matthews_correlation: 0.6711 - val_loss: 0.0924 - val_matthews_correlation: 0.6712\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.71199\n",
      "Epoch 8/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1021 - matthews_correlation: 0.6569 - val_loss: 0.0863 - val_matthews_correlation: 0.6858\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.71199\n",
      "Epoch 9/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0992 - matthews_correlation: 0.7051 - val_loss: 0.0903 - val_matthews_correlation: 0.5858\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.71199\n",
      "Epoch 10/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0980 - matthews_correlation: 0.6327 - val_loss: 0.0907 - val_matthews_correlation: 0.7228\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.71199 to 0.72277, saving model to weights_4.h5\n",
      "Epoch 11/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.1000 - matthews_correlation: 0.6071 - val_loss: 0.0877 - val_matthews_correlation: 0.6098\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.72277\n",
      "Epoch 12/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0945 - matthews_correlation: 0.6526 - val_loss: 0.0880 - val_matthews_correlation: 0.6562\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.72277\n",
      "Epoch 13/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0946 - matthews_correlation: 0.6918 - val_loss: 0.0890 - val_matthews_correlation: 0.6570\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.72277\n",
      "Epoch 14/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0961 - matthews_correlation: 0.6670 - val_loss: 0.0959 - val_matthews_correlation: 0.7241\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.72277 to 0.72410, saving model to weights_4.h5\n",
      "Epoch 15/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0917 - matthews_correlation: 0.7103 - val_loss: 0.0827 - val_matthews_correlation: 0.7179\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.72410\n",
      "Epoch 16/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0997 - matthews_correlation: 0.6474 - val_loss: 0.0882 - val_matthews_correlation: 0.7086\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.72410\n",
      "Epoch 17/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0970 - matthews_correlation: 0.6658 - val_loss: 0.0868 - val_matthews_correlation: 0.6761\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.72410\n",
      "Epoch 18/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0914 - matthews_correlation: 0.6818 - val_loss: 0.0820 - val_matthews_correlation: 0.7011\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.72410\n",
      "Epoch 19/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0894 - matthews_correlation: 0.7043 - val_loss: 0.0825 - val_matthews_correlation: 0.7624\n",
      "\n",
      "Epoch 00019: val_matthews_correlation improved from 0.72410 to 0.76238, saving model to weights_4.h5\n",
      "Epoch 20/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0915 - matthews_correlation: 0.6840 - val_loss: 0.0841 - val_matthews_correlation: 0.6951\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 21/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0924 - matthews_correlation: 0.6947 - val_loss: 0.0900 - val_matthews_correlation: 0.5922\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 22/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0872 - matthews_correlation: 0.7081 - val_loss: 0.0812 - val_matthews_correlation: 0.7576\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 23/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0877 - matthews_correlation: 0.7096 - val_loss: 0.0889 - val_matthews_correlation: 0.6052\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 24/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0899 - matthews_correlation: 0.7098 - val_loss: 0.0872 - val_matthews_correlation: 0.6147\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 25/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0856 - matthews_correlation: 0.7124 - val_loss: 0.0843 - val_matthews_correlation: 0.7512\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 26/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0862 - matthews_correlation: 0.6997 - val_loss: 0.0800 - val_matthews_correlation: 0.7181\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 27/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0835 - matthews_correlation: 0.7266 - val_loss: 0.0766 - val_matthews_correlation: 0.7402\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 28/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0861 - matthews_correlation: 0.7082 - val_loss: 0.0866 - val_matthews_correlation: 0.6924\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 29/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0868 - matthews_correlation: 0.6999 - val_loss: 0.0803 - val_matthews_correlation: 0.7464\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 30/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0858 - matthews_correlation: 0.7232 - val_loss: 0.0896 - val_matthews_correlation: 0.6400\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 31/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0923 - matthews_correlation: 0.6797 - val_loss: 0.0834 - val_matthews_correlation: 0.7197\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 32/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0868 - matthews_correlation: 0.7144 - val_loss: 0.0788 - val_matthews_correlation: 0.6853\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 33/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0869 - matthews_correlation: 0.6899 - val_loss: 0.0820 - val_matthews_correlation: 0.7178\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 34/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0850 - matthews_correlation: 0.7124 - val_loss: 0.0895 - val_matthews_correlation: 0.7515\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0900 - matthews_correlation: 0.7148 - val_loss: 0.0804 - val_matthews_correlation: 0.7515\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 36/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0856 - matthews_correlation: 0.7347 - val_loss: 0.0853 - val_matthews_correlation: 0.6787\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 37/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0877 - matthews_correlation: 0.6836 - val_loss: 0.0893 - val_matthews_correlation: 0.6002\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 38/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0860 - matthews_correlation: 0.7147 - val_loss: 0.0830 - val_matthews_correlation: 0.7569\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 39/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0829 - matthews_correlation: 0.7237 - val_loss: 0.0812 - val_matthews_correlation: 0.7515\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 40/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0805 - matthews_correlation: 0.7372 - val_loss: 0.0768 - val_matthews_correlation: 0.7008\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 41/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0832 - matthews_correlation: 0.7061 - val_loss: 0.0797 - val_matthews_correlation: 0.7448\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 42/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0855 - matthews_correlation: 0.7000 - val_loss: 0.0816 - val_matthews_correlation: 0.7197\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 43/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0802 - matthews_correlation: 0.7343 - val_loss: 0.0755 - val_matthews_correlation: 0.7396\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 44/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0815 - matthews_correlation: 0.7047 - val_loss: 0.0799 - val_matthews_correlation: 0.7232\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 45/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0802 - matthews_correlation: 0.7228 - val_loss: 0.0846 - val_matthews_correlation: 0.7335\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 46/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0809 - matthews_correlation: 0.7173 - val_loss: 0.0834 - val_matthews_correlation: 0.7554\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 47/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0815 - matthews_correlation: 0.7132 - val_loss: 0.0884 - val_matthews_correlation: 0.6390\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 48/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0835 - matthews_correlation: 0.6953 - val_loss: 0.0763 - val_matthews_correlation: 0.7425\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 49/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0785 - matthews_correlation: 0.7219 - val_loss: 0.0786 - val_matthews_correlation: 0.7430\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.76238\n",
      "Epoch 50/50\n",
      "6971/6971 [==============================] - 17s 2ms/step - loss: 0.0867 - matthews_correlation: 0.7009 - val_loss: 0.0774 - val_matthews_correlation: 0.7412\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.76238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8712,), (8712,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "# First, create a set of indexes of the 5 folds\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    model.fit(train_X, train_y, batch_size=256, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "preds_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tmp/y_val_tmp_8.npy', y_val)\n",
    "np.save('./tmp/preds_val_tmp_8.npy', preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
   },
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    \n",
    "    #y_pred = K.cast(y_pred, np.float)\n",
    "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = np.sum(y_pos * y_pred_pos)\n",
    "    tn = np.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = np.sum(y_neg * y_pred_pos)\n",
    "    fn = np.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
   },
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        #score = matthews_correlation(y_true, (y_proba > threshold).astype(int))\n",
    "        #score = K.eval(matthews_correlation(y_true, (y_proba > threshold).astype(int)))\n",
    "        #score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        score = matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "6fee7f722ed08bc1453a822a4371ed2d48e08abc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 657.65it/s]\n"
     ]
    }
   ],
   "source": [
    "best_threshold = threshold_search(y_val, preds_val)['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301185618158551"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_correlation(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 2.97 ms, total: 14.7 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now load the test data\n",
    "# This first part is the meta data, not the main data, the measurements\n",
    "meta_test = pd.read_csv('../input/metadata_test.csv')\n",
    "df_test = meta_test.set_index(['id_measurement', 'phase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_measurement  phase\n",
       "signal_id                       \n",
       "8712                 2904      0\n",
       "8713                 2904      1\n",
       "8714                 2904      2\n",
       "8715                 2905      0\n",
       "8716                 2905      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test = meta_test.set_index(['signal_id'])\n",
    "meta_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data_test(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start+8712, end+8712)]).to_pandas()\n",
    "    X = []\n",
    "\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_test.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id = df_test.loc[id_measurement].loc[phase][0]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            #if phase == 0:\n",
    "                #ts_1 = min_max_transf(praq_test[str(signal_id)], min_data=min_num, max_data=max_num)\n",
    "                #ts_2 = min_max_transf(praq_test[str(signal_id+1)], min_data=min_num, max_data=max_num)\n",
    "                #ts_3 = min_max_transf(praq_test[str(signal_id+2)], min_data=min_num, max_data=max_num)\n",
    "                \n",
    "                #ts_wave_1 = denoise_signal(ts_1, wavelet='haar', level=1)\n",
    "                #ts_wave_2 = denoise_signal(ts_2, wavelet='haar', level=1)\n",
    "                #ts_wave_3 = denoise_signal(ts_3, wavelet='haar', level=1)\n",
    "                \n",
    "                #ts_rm_1, _ = remove_corona(ts_wave_1)\n",
    "                #ts_rm_2, _ = remove_corona(ts_wave_2)\n",
    "                #ts_rm_3, _ = remove_corona(ts_wave_3)\n",
    "                \n",
    "                #ts_sum = ts_rm_1 + ts_rm_2 + ts_rm_3\n",
    "                #X_signal.append(transform_ts_sum(ts_sum))\n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtest(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X = prep_data_test(start, end)\n",
    "    return idx, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "num_cores = 16 \n",
    "#def load_all():\n",
    "total_size = len(meta_test)\n",
    "chunk_size = np.ceil(total_size/num_cores)\n",
    "#train_size = len(df_train)\n",
    "\n",
    "for i in range(16):\n",
    "    if i != 15:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(start_idx + chunk_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "    else:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(total_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 56/424 [00:23<02:16,  2.69it/s]Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ad23c843b30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_subtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresults_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 25, in transform_ts\n",
      "    percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4291, in percentile\n",
      "    interpolation=interpolation)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4033, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4392, in _percentile\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 25, in transform_ts\n",
      "    percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100])\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4291, in percentile\n",
      "    interpolation=interpolation)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4033, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4033, in _ureduce\n",
      "    r = func(a, **kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4392, in _percentile\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 3103, in get_value\n",
      "    tz=getattr(series.dtype, 'tz', None))\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 17, in transform_ts\n",
      "    ts_range = ts_std[i:i + bucket_size]\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"pandas/_libs/index.pyx\", line 106, in pandas._libs.index.IndexEngine.get_value\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 764, in __getitem__\n",
      "    key = com._apply_if_callable(key, self)\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 25, in transform_ts\n",
      "    percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100])\n",
      "  File \"pandas/_libs/index.pyx\", line 114, in pandas._libs.index.IndexEngine.get_value\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"pandas/_libs/index.pyx\", line 142, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4291, in percentile\n",
      "    interpolation=interpolation)\n",
      "TypeError: 'slice(230000, 235000, None)' is an invalid key\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 20, in transform_ts\n",
      "    mean = ts_range.mean()\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 4392, in _percentile\n",
      "    ap.partition(concatenate((indices_below, indices_above)), axis=axis)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\", line 9589, in stat_func\n",
      "    numeric_only=numeric_only)\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 20, in transform_ts\n",
      "    mean = ts_range.mean()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 3218, in _reduce\n",
      "    return op(delegate, skipna=skipna, **kwds)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\", line 9589, in stat_func\n",
      "    numeric_only=numeric_only)\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\", line 77, in _f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 3218, in _reduce\n",
      "    return op(delegate, skipna=skipna, **kwds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\", line 121, in f\n",
      "    result = bn_func(values, axis=axis, **kwds)\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 8, in transform_ts\n",
      "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
      "  File \"<ipython-input-7-eb80ff59fcda>\", line 9, in min_max_transf\n",
      "    return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\", line 77, in _f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<ipython-input-30-117767a5cd96>\", line 3, in process_subtest\n",
      "    X = prep_data_test(start, end)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/nanops.py\", line 121, in f\n",
      "    result = bn_func(values, axis=axis, **kwds)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 1066, in wrapper\n",
      "    result = safe_na_op(lvalues, rvalues)\n",
      "  File \"<ipython-input-29-4f0a87469d6c>\", line 34, in prep_data_test\n",
      "    X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
      "  File \"<ipython-input-12-8f63d5952239>\", line 17, in transform_ts\n",
      "    ts_range = ts_std[i:i + bucket_size]\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 1030, in safe_na_op\n",
      "    return na_op(lvalues, rvalues)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\", line 766, in __getitem__\n",
      "    result = self.index.get_value(self, key)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 1009, in na_op\n",
      "    result = expressions.evaluate(op, str_rep, x, y, **eval_kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 3122, in get_value\n",
      "    if is_scalar(key):  # pragma: no cover\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\", line 205, in evaluate\n",
      "    return _evaluate(op, op_str, a, b, **eval_kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\", line 111, in _evaluate_numexpr\n",
      "    **eval_kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/site-packages/numexpr/necompiler.py\", line 821, in evaluate\n",
      "    return compiled_ex(*arguments, **kwargs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "pool = Pool()\n",
    "results_1 = pool.map(process_subtest, all_chunks[0:8])    \n",
    "results_1 = sorted(results_1, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = pool.map(process_subtest, all_chunks[8:16])    \n",
    "results_2 = sorted(results_2, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_1 + results_2\n",
    "X_test = np.concatenate([item[1] for item in results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_7.npy\",X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load('./X_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6779, 160, 57)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(len(submission))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 5s 675us/step\n",
      "6779/6779 [==============================] - 5s 689us/step\n",
      "6779/6779 [==============================] - 5s 695us/step\n",
      "6779/6779 [==============================] - 5s 689us/step\n",
      "6779/6779 [==============================] - 5s 692us/step\n"
     ]
    }
   ],
   "source": [
    "preds_test = []\n",
    "for i in range(5):\n",
    "    model.load_weights('weights_{}.h5'.format(i))\n",
    "    pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "    pred_3 = []\n",
    "    for pred_scalar in pred:\n",
    "        for i in range(3):\n",
    "            pred_3.append(pred_scalar)\n",
    "    preds_test.append(pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "9f76c471eaf983707d446c5081ab3d50c4e40ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20337,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_2 = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\n",
    "preds_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "b35723f85d494b4b6ec630dd7c79135a110a4062"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'] = preds_test_2\n",
    "submission.to_csv('../output/submission_28.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7600d0093a9880003240ef9ce0a1f1303e4d982"
   },
   "outputs": [],
   "source": [
    "lstm_preds = np.squeeze(np.mean(preds_test, axis=0))\n",
    "np.save(\"./lstm_preds.npy\", lstm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds_test_2[preds_test_2==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19404\n",
       "1      933\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 17s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights_0.h5')\n",
    "pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "pred_3 = []\n",
    "for pred_scalar in pred:\n",
    "    for i in range(3):\n",
    "        pred_3.append(pred_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5823f4244eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_3\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "pred_3>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3 = (np.squeeze(pred_3) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds_3[preds_3==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = preds_3\n",
    "submission.to_csv('../output/submission_22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19128\n",
       "1     1209\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
