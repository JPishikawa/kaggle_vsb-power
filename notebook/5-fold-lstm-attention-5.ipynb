{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
   },
   "source": [
    "### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.\n",
    "### If any part is not clear, please comment.  \n",
    "### Please upvote if it was helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwademo123/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
    "\n",
    "from scipy.signal import chirp, find_peaks, peak_widths\n",
    "import pywt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
   },
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('../input/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
   },
   "outputs": [],
   "source": [
    "# in other notebook I have extracted the min and max values from the train data, the measurements\n",
    "max_num = 127\n",
    "min_num = -128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
   },
   "outputs": [],
   "source": [
    "# This function standardize the data from (-128 to 127) to (-1 to 1)\n",
    "# Theoretically it helps in the NN Model training, but I didn't tested without it\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddest(d, axis=None):\n",
    "    \"\"\"\n",
    "    Mean Absolute Deviation\n",
    "    \"\"\"\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal( x, wavelet='db4', level=1):\n",
    "    \"\"\"\n",
    "    1. Adapted from waveletSmooth function found here:\n",
    "    http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "    2. Threshold equation and using hard mode in threshold as mentioned\n",
    "    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "    http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\", level=level)\n",
    "    \n",
    "    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "\n",
    "    # Calculte the univeral threshold\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec( coeff[1:], wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corona(x_dn, maxDistance=10, maxHeightRatio=0.25, maxTicksRemoval=500):\n",
    "    index = pd.Series(x_dn).loc[np.abs(x_dn)>0].index\n",
    "    corona_idx = []\n",
    "    for idx in index:\n",
    "        for i in range(1,maxDistance+1):\n",
    "            if idx+i < pd.Series(x_dn).shape[0]:\n",
    "                if x_dn[idx+i]/(x_dn[idx]+1e-04)<-maxHeightRatio:\n",
    "                    x_dn[idx:idx+maxTicksRemoval] = 0\n",
    "                    corona_idx.append(idx)\n",
    "    return x_dn, corona_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    ts_wave = denoise_signal(ts_std, wavelet='haar', level=1)\n",
    "    ts_rm, corona_idx = remove_corona(ts_wave)\n",
    "    \n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        ts_rm_range = ts_rm[int(i/2):int(i/2) + int(bucket_size/2)]\n",
    "        #ts_rm_range = pd.Series(ts_rm_range)\n",
    "        \n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        \n",
    "        #numpeaks = ts_rm_range[ts_rm_range!=0].count()\n",
    "        numpeaks = np.sum(ts_rm_range!=0)\n",
    "        #numpospeaks = ts_rm_range[ts_rm_range>0].count()\n",
    "        numpospeaks = np.sum(ts_rm_range>0)\n",
    "        #numnegpeaks = ts_rm_range[ts_rm_range<0].count()\n",
    "        numnegpeaks = np.sum(ts_rm_range<0)\n",
    "\n",
    "        meanamp = np.mean(ts_rm_range)\n",
    "        meanamppos = np.mean(ts_rm_range[ts_rm_range>0])\n",
    "        meanampneg = np.mean(ts_rm_range[ts_rm_range<0])\n",
    "\n",
    "        maxamp = np.max(ts_rm_range)\n",
    "        minamp = np.min(ts_rm_range)\n",
    "        \n",
    "        peaks, _ = find_peaks(ts_rm_range)\n",
    "        results_full = peak_widths(ts_rm_range, peaks, rel_height=1)\n",
    "        \n",
    "        if len(results_full[0])==0:\n",
    "            maxwidth = 0\n",
    "            minwidth = 0\n",
    "        else:\n",
    "            maxwidth = np.max(results_full[0])\n",
    "            minwidth = np.min(results_full[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        feat_array = np.asarray([mean, std, std_top, std_bot, max_range])\n",
    "        feat_array_2 = np.asarray([numpeaks, numpospeaks, numnegpeaks, meanamp, meanamppos, meanampneg,\n",
    "                                 maxamp, minamp, maxwidth, minwidth])\n",
    "        \n",
    "        new_ts.append(np.concatenate([feat_array, feat_array_2, percentil_calc, relative_percentile]))\n",
    "        \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    new_ts = np.asarray(new_ts)\n",
    "    new_ts[np.isnan(new_ts)] = 0\n",
    "    \n",
    "    new_ts = scaler.fit_transform(new_ts)\n",
    "        \n",
    "    return new_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    X = []\n",
    "    y = []\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)\n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_train[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtrain(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X, y = prep_data(start, end)\n",
    "    return idx, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_cores = 8 \n",
    "all_chunks = []\n",
    "\n",
    "total_size = len(df_train)\n",
    "chunk_size = total_size/num_cores\n",
    "\n",
    "for i in range(8):\n",
    "    start_idx = int(i * chunk_size)\n",
    "    end_idx = int(start_idx + chunk_size)\n",
    "    chunk = (start_idx, end_idx, i)\n",
    "    all_chunks.append(chunk)\n",
    "    \n",
    "all_chunks[0]\n",
    "\n",
    "pool = Pool()\n",
    "results = pool.map(process_subtrain, all_chunks)\n",
    "\n",
    "sorted(results, key=lambda tup: tup[0])\n",
    "\n",
    "np.concatenate([item[1][1] for item in results], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [14:13<00:00,  3.00s/it]\n",
      "100%|██████████| 363/363 [14:34<00:00,  2.87s/it]\n",
      "100%|██████████| 363/363 [15:07<00:00,  1.79s/it]\n",
      "100%|██████████| 363/363 [15:12<00:00,  2.09s/it]\n",
      "100%|██████████| 363/363 [15:26<00:00,  1.40it/s]\n",
      "100%|██████████| 363/363 [15:28<00:00,  1.86s/it]\n",
      "100%|██████████| 363/363 [15:35<00:00,  1.16s/it]\n",
      "100%|██████████| 363/363 [15:52<00:00,  1.27s/it]\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/cwademo123/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# this code is very simple, divide the total size of the df_train into two sets and process it\n",
    "#X = []\n",
    "#y = []\n",
    "all_chunks = []\n",
    "\n",
    "num_cores = 8 \n",
    "#def load_all():\n",
    "total_size = len(df_train)\n",
    "chunk_size = total_size/num_cores\n",
    "\n",
    "for i in range(8):\n",
    "    start_idx = int(i * chunk_size)\n",
    "    end_idx = int(start_idx + chunk_size)\n",
    "    chunk = (start_idx, end_idx, i)\n",
    "    all_chunks.append(chunk)\n",
    "\n",
    "pool = Pool()\n",
    "results = pool.map(process_subtrain, all_chunks)    \n",
    "results = sorted(results, key=lambda tup: tup[0])\n",
    "\n",
    "X = np.concatenate([item[1] for item in results], axis=0)\n",
    "y = np.concatenate([item[2] for item in results], axis=0)\n",
    "\n",
    "#load_all()\n",
    "\n",
    "#X = np.asarray(X)\n",
    "#y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 160, 87) (2904,)\n"
     ]
    }
   ],
   "source": [
    "# The X shape here is very important. It is also important undertand a little how a LSTM works\n",
    "# X.shape[0] is the number of id_measuremts contained in train data\n",
    "# X.shape[1] is the number of chunks resultant of the transformation, each of this date enters in the LSTM serialized\n",
    "# This way the LSTM can understand the position of a data relative with other and activate a signal that needs\n",
    "# a serie of inputs in a specifc order.\n",
    "# X.shape[3] is the number of features multiplied by the number of phases (3)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
   },
   "outputs": [],
   "source": [
    "# save data into file, a numpy specific format\n",
    "np.save(\"X_2.npy\",X)\n",
    "np.save(\"y_2.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./X_2.npy\")\n",
    "y = np.load(\"./y_2.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X[np.isnan(X)] = 0\n",
    "\n",
    "X = min_max_transf(X, np.min(X), np.max(X), range_needed=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "289bc7d1ab8048a60025801b457f8df1d848acbc"
   },
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    \n",
    "    \n",
    "    # This is the LSTM layer\n",
    "    # Bidirecional implies that the 160 chunks are calculated in both ways, 0 to 159 and 159 to zero\n",
    "    # although it appear that just 0 to 159 way matter, I have tested with and without, and tha later worked best\n",
    "    # 128 and 64 are the number of cells used, too many can overfit and too few can underfit\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n",
    "    # The second LSTM can give more fire power to the model, but can overfit it too\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    # Attention is a new tecnology that can be applyed to a Recurrent NN to give more meanings to a signal found in the middle\n",
    "    # of the data, it helps more in longs chains of data. A normal RNN give all the responsibility of detect the signal\n",
    "    # to the last cell. Google RNN Attention for more information :)\n",
    "    x = Attention(input_shape[1])(x)\n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    x = Dense(64)(x)\n",
    "    \n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 2322 samples, validate on 582 samples\n",
      "Epoch 1/50\n",
      "2322/2322 [==============================] - 29s 12ms/step - loss: 0.2620 - matthews_correlation: -0.0043 - val_loss: 0.1858 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_0.h5\n",
      "Epoch 2/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1598 - matthews_correlation: 0.0273 - val_loss: 0.1297 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1333 - matthews_correlation: 0.3086 - val_loss: 0.1406 - val_matthews_correlation: -0.0039\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1260 - matthews_correlation: 0.3577 - val_loss: 0.1191 - val_matthews_correlation: 0.6323\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.00000 to 0.63232, saving model to weights_0.h5\n",
      "Epoch 5/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1189 - matthews_correlation: 0.4567 - val_loss: 0.0886 - val_matthews_correlation: 0.6893\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.63232 to 0.68929, saving model to weights_0.h5\n",
      "Epoch 6/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1243 - matthews_correlation: 0.5908 - val_loss: 0.0993 - val_matthews_correlation: 0.6893\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.68929\n",
      "Epoch 7/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.1250 - matthews_correlation: 0.4843 - val_loss: 0.0936 - val_matthews_correlation: 0.5917\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.68929\n",
      "Epoch 8/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1045 - matthews_correlation: 0.5330 - val_loss: 0.0778 - val_matthews_correlation: 0.7259\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.68929 to 0.72594, saving model to weights_0.h5\n",
      "Epoch 9/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0952 - matthews_correlation: 0.6756 - val_loss: 0.0839 - val_matthews_correlation: 0.6872\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.72594\n",
      "Epoch 10/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0981 - matthews_correlation: 0.6300 - val_loss: 0.0820 - val_matthews_correlation: 0.7148\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.72594\n",
      "Epoch 11/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0905 - matthews_correlation: 0.6928 - val_loss: 0.0784 - val_matthews_correlation: 0.6999\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.72594\n",
      "Epoch 12/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0858 - matthews_correlation: 0.6845 - val_loss: 0.0785 - val_matthews_correlation: 0.7587\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.72594 to 0.75871, saving model to weights_0.h5\n",
      "Epoch 13/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0820 - matthews_correlation: 0.7375 - val_loss: 0.0794 - val_matthews_correlation: 0.6988\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.75871\n",
      "Epoch 14/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0944 - matthews_correlation: 0.6380 - val_loss: 0.0856 - val_matthews_correlation: 0.7595\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.75871 to 0.75949, saving model to weights_0.h5\n",
      "Epoch 15/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0869 - matthews_correlation: 0.6847 - val_loss: 0.0896 - val_matthews_correlation: 0.7453\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.75949\n",
      "Epoch 16/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0894 - matthews_correlation: 0.6827 - val_loss: 0.0943 - val_matthews_correlation: 0.6891\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.75949\n",
      "Epoch 17/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.1311 - matthews_correlation: 0.5267 - val_loss: 0.0998 - val_matthews_correlation: 0.6112\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.75949\n",
      "Epoch 18/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0810 - matthews_correlation: 0.7146 - val_loss: 0.0812 - val_matthews_correlation: 0.7441\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.75949\n",
      "Epoch 19/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0674 - matthews_correlation: 0.8159 - val_loss: 0.0873 - val_matthews_correlation: 0.6560\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.75949\n",
      "Epoch 20/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0714 - matthews_correlation: 0.7451 - val_loss: 0.0753 - val_matthews_correlation: 0.8337\n",
      "\n",
      "Epoch 00020: val_matthews_correlation improved from 0.75949 to 0.83370, saving model to weights_0.h5\n",
      "Epoch 21/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0580 - matthews_correlation: 0.8362 - val_loss: 0.0911 - val_matthews_correlation: 0.6570\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 22/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0536 - matthews_correlation: 0.8470 - val_loss: 0.0900 - val_matthews_correlation: 0.6956\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 23/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0554 - matthews_correlation: 0.8245 - val_loss: 0.1175 - val_matthews_correlation: 0.5778\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 24/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0735 - matthews_correlation: 0.7400 - val_loss: 0.0921 - val_matthews_correlation: 0.6512\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 25/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0591 - matthews_correlation: 0.7773 - val_loss: 0.1322 - val_matthews_correlation: 0.6449\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 26/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0583 - matthews_correlation: 0.8262 - val_loss: 0.0854 - val_matthews_correlation: 0.7153\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 27/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0453 - matthews_correlation: 0.8849 - val_loss: 0.2352 - val_matthews_correlation: 0.5861\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 28/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0635 - matthews_correlation: 0.7587 - val_loss: 0.1060 - val_matthews_correlation: 0.6444\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 29/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0541 - matthews_correlation: 0.8119 - val_loss: 0.0909 - val_matthews_correlation: 0.6889\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 30/50\n",
      "2322/2322 [==============================] - 22s 10ms/step - loss: 0.0463 - matthews_correlation: 0.8590 - val_loss: 0.1017 - val_matthews_correlation: 0.7230\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 31/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0563 - matthews_correlation: 0.8210 - val_loss: 0.1038 - val_matthews_correlation: 0.7213\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 32/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0407 - matthews_correlation: 0.9039 - val_loss: 0.1522 - val_matthews_correlation: 0.6360\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 33/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0327 - matthews_correlation: 0.9199 - val_loss: 0.0948 - val_matthews_correlation: 0.7919\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 34/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0310 - matthews_correlation: 0.9218 - val_loss: 0.1202 - val_matthews_correlation: 0.7138\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 35/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0266 - matthews_correlation: 0.9482 - val_loss: 0.1040 - val_matthews_correlation: 0.7363\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 36/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0248 - matthews_correlation: 0.9277 - val_loss: 0.1847 - val_matthews_correlation: 0.6652\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 37/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0488 - matthews_correlation: 0.8676 - val_loss: 0.1100 - val_matthews_correlation: 0.6736\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 38/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0333 - matthews_correlation: 0.8962 - val_loss: 0.1231 - val_matthews_correlation: 0.6363\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 39/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0259 - matthews_correlation: 0.9374 - val_loss: 0.1186 - val_matthews_correlation: 0.6892\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 40/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0310 - matthews_correlation: 0.9247 - val_loss: 0.1619 - val_matthews_correlation: 0.6027\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 41/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0289 - matthews_correlation: 0.9304 - val_loss: 0.1086 - val_matthews_correlation: 0.7157\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 42/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0222 - matthews_correlation: 0.9140 - val_loss: 0.1375 - val_matthews_correlation: 0.6719\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 43/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0193 - matthews_correlation: 0.9355 - val_loss: 0.1514 - val_matthews_correlation: 0.6956\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 44/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0153 - matthews_correlation: 0.9560 - val_loss: 0.1784 - val_matthews_correlation: 0.6721\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 45/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0184 - matthews_correlation: 0.9547 - val_loss: 0.1492 - val_matthews_correlation: 0.7419\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 46/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0254 - matthews_correlation: 0.9168 - val_loss: 0.2692 - val_matthews_correlation: 0.6128\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 47/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0815 - matthews_correlation: 0.7876 - val_loss: 0.1028 - val_matthews_correlation: 0.6000\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 48/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0392 - matthews_correlation: 0.8888 - val_loss: 0.1355 - val_matthews_correlation: 0.6810\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 49/50\n",
      "2322/2322 [==============================] - 21s 9ms/step - loss: 0.0231 - matthews_correlation: 0.9524 - val_loss: 0.1642 - val_matthews_correlation: 0.6638\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.83370\n",
      "Epoch 50/50\n",
      "2322/2322 [==============================] - 22s 9ms/step - loss: 0.0241 - matthews_correlation: 0.9227 - val_loss: 0.1176 - val_matthews_correlation: 0.7250\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.83370\n",
      "Beginning fold 2\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 25s 11ms/step - loss: 0.2596 - matthews_correlation: 0.0031 - val_loss: 0.2138 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_1.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1928 - matthews_correlation: 0.0000e+00 - val_loss: 0.1572 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1457 - matthews_correlation: 0.2283 - val_loss: 0.1499 - val_matthews_correlation: 0.2358\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.23577, saving model to weights_1.h5\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1223 - matthews_correlation: 0.5800 - val_loss: 0.1369 - val_matthews_correlation: 0.4206\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.23577 to 0.42056, saving model to weights_1.h5\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1197 - matthews_correlation: 0.5189 - val_loss: 0.1302 - val_matthews_correlation: 0.4031\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.42056\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1082 - matthews_correlation: 0.6431 - val_loss: 0.1254 - val_matthews_correlation: 0.4684\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.42056 to 0.46842, saving model to weights_1.h5\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0988 - matthews_correlation: 0.7028 - val_loss: 0.1255 - val_matthews_correlation: 0.4610\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.46842\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0945 - matthews_correlation: 0.6935 - val_loss: 0.1277 - val_matthews_correlation: 0.4666\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.46842\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0933 - matthews_correlation: 0.7053 - val_loss: 0.1229 - val_matthews_correlation: 0.5312\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.46842 to 0.53116, saving model to weights_1.h5\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0922 - matthews_correlation: 0.6921 - val_loss: 0.1515 - val_matthews_correlation: 0.4347\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.53116\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1067 - matthews_correlation: 0.6791 - val_loss: 0.1364 - val_matthews_correlation: 0.5333\n",
      "\n",
      "Epoch 00011: val_matthews_correlation improved from 0.53116 to 0.53335, saving model to weights_1.h5\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0908 - matthews_correlation: 0.7218 - val_loss: 0.1236 - val_matthews_correlation: 0.4934\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.53335\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0799 - matthews_correlation: 0.7547 - val_loss: 0.1131 - val_matthews_correlation: 0.6075\n",
      "\n",
      "Epoch 00013: val_matthews_correlation improved from 0.53335 to 0.60751, saving model to weights_1.h5\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0777 - matthews_correlation: 0.7561 - val_loss: 0.1091 - val_matthews_correlation: 0.5722\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.60751\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0801 - matthews_correlation: 0.7262 - val_loss: 0.1116 - val_matthews_correlation: 0.6228\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.60751 to 0.62283, saving model to weights_1.h5\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0771 - matthews_correlation: 0.7532 - val_loss: 0.1406 - val_matthews_correlation: 0.5719\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0815 - matthews_correlation: 0.7554 - val_loss: 0.1154 - val_matthews_correlation: 0.5513\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0722 - matthews_correlation: 0.7837 - val_loss: 0.1471 - val_matthews_correlation: 0.4815\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0670 - matthews_correlation: 0.7746 - val_loss: 0.1197 - val_matthews_correlation: 0.5898\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0645 - matthews_correlation: 0.7824 - val_loss: 0.1412 - val_matthews_correlation: 0.5556\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0633 - matthews_correlation: 0.7751 - val_loss: 0.1238 - val_matthews_correlation: 0.6145\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0594 - matthews_correlation: 0.7922 - val_loss: 0.1045 - val_matthews_correlation: 0.5590\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.62283\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0485 - matthews_correlation: 0.8314 - val_loss: 0.1156 - val_matthews_correlation: 0.6303\n",
      "\n",
      "Epoch 00023: val_matthews_correlation improved from 0.62283 to 0.63035, saving model to weights_1.h5\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0473 - matthews_correlation: 0.8802 - val_loss: 0.1442 - val_matthews_correlation: 0.5926\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.63035\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0561 - matthews_correlation: 0.8383 - val_loss: 0.1493 - val_matthews_correlation: 0.5526\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.63035\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0501 - matthews_correlation: 0.8522 - val_loss: 0.1256 - val_matthews_correlation: 0.6144\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.63035\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0441 - matthews_correlation: 0.8575 - val_loss: 0.1906 - val_matthews_correlation: 0.5052\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.63035\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0519 - matthews_correlation: 0.8508 - val_loss: 0.1419 - val_matthews_correlation: 0.6437\n",
      "\n",
      "Epoch 00028: val_matthews_correlation improved from 0.63035 to 0.64369, saving model to weights_1.h5\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0691 - matthews_correlation: 0.8196 - val_loss: 0.1458 - val_matthews_correlation: 0.5800\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.64369\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0448 - matthews_correlation: 0.8536 - val_loss: 0.1348 - val_matthews_correlation: 0.5301\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.64369\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0431 - matthews_correlation: 0.8791 - val_loss: 0.1463 - val_matthews_correlation: 0.6468\n",
      "\n",
      "Epoch 00031: val_matthews_correlation improved from 0.64369 to 0.64675, saving model to weights_1.h5\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0363 - matthews_correlation: 0.8912 - val_loss: 0.1381 - val_matthews_correlation: 0.5807\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0252 - matthews_correlation: 0.9290 - val_loss: 0.1635 - val_matthews_correlation: 0.6454\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0230 - matthews_correlation: 0.9268 - val_loss: 0.1748 - val_matthews_correlation: 0.6013\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0776 - matthews_correlation: 0.7643 - val_loss: 0.1392 - val_matthews_correlation: 0.5364\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0516 - matthews_correlation: 0.8313 - val_loss: 0.1447 - val_matthews_correlation: 0.6153\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0391 - matthews_correlation: 0.8500 - val_loss: 0.1436 - val_matthews_correlation: 0.5607\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0567 - matthews_correlation: 0.8262 - val_loss: 0.1448 - val_matthews_correlation: 0.6197\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.64675\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0375 - matthews_correlation: 0.8872 - val_loss: 0.1374 - val_matthews_correlation: 0.6730\n",
      "\n",
      "Epoch 00039: val_matthews_correlation improved from 0.64675 to 0.67304, saving model to weights_1.h5\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0281 - matthews_correlation: 0.9037 - val_loss: 0.1732 - val_matthews_correlation: 0.6647\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.67304\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0269 - matthews_correlation: 0.9430 - val_loss: 0.1702 - val_matthews_correlation: 0.5887\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.67304\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0200 - matthews_correlation: 0.9335 - val_loss: 0.1917 - val_matthews_correlation: 0.4926\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.67304\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0245 - matthews_correlation: 0.9324 - val_loss: 0.2252 - val_matthews_correlation: 0.6778\n",
      "\n",
      "Epoch 00043: val_matthews_correlation improved from 0.67304 to 0.67775, saving model to weights_1.h5\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0265 - matthews_correlation: 0.9283 - val_loss: 0.1641 - val_matthews_correlation: 0.6116\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0128 - matthews_correlation: 0.9704 - val_loss: 0.2036 - val_matthews_correlation: 0.6331\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0107 - matthews_correlation: 0.9805 - val_loss: 0.2050 - val_matthews_correlation: 0.6081\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0184 - matthews_correlation: 0.8962 - val_loss: 0.1874 - val_matthews_correlation: 0.5624\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0181 - matthews_correlation: 0.9417 - val_loss: 0.1924 - val_matthews_correlation: 0.6102\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0118 - matthews_correlation: 0.9649 - val_loss: 0.2157 - val_matthews_correlation: 0.5824\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.67775\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0135 - matthews_correlation: 0.9647 - val_loss: 0.1998 - val_matthews_correlation: 0.5862\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.67775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 3\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 25s 11ms/step - loss: 0.3035 - matthews_correlation: 0.0000e+00 - val_loss: 0.2284 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_2.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 22s 10ms/step - loss: 0.2220 - matthews_correlation: 0.0000e+00 - val_loss: 0.2057 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1745 - matthews_correlation: -4.5439e-04 - val_loss: 0.1718 - val_matthews_correlation: 0.3448\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.34475, saving model to weights_2.h5\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1387 - matthews_correlation: 0.2617 - val_loss: 0.1532 - val_matthews_correlation: 0.3782\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.34475 to 0.37818, saving model to weights_2.h5\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1275 - matthews_correlation: 0.3149 - val_loss: 0.1547 - val_matthews_correlation: 0.6135\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.37818 to 0.61348, saving model to weights_2.h5\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.1440 - matthews_correlation: 0.4359 - val_loss: 0.1249 - val_matthews_correlation: 0.5293\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.61348\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1151 - matthews_correlation: 0.4450 - val_loss: 0.1136 - val_matthews_correlation: 0.6233\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.61348 to 0.62333, saving model to weights_2.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1009 - matthews_correlation: 0.5934 - val_loss: 0.1210 - val_matthews_correlation: 0.6179\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.1015 - matthews_correlation: 0.6229 - val_loss: 0.1071 - val_matthews_correlation: 0.4683\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0934 - matthews_correlation: 0.6778 - val_loss: 0.1251 - val_matthews_correlation: 0.3388\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.1045 - matthews_correlation: 0.6176 - val_loss: 0.1048 - val_matthews_correlation: 0.4836\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0863 - matthews_correlation: 0.7041 - val_loss: 0.1116 - val_matthews_correlation: 0.4815\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0862 - matthews_correlation: 0.6777 - val_loss: 0.1056 - val_matthews_correlation: 0.5603\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.62333\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0880 - matthews_correlation: 0.6838 - val_loss: 0.1012 - val_matthews_correlation: 0.6379\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.62333 to 0.63786, saving model to weights_2.h5\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0740 - matthews_correlation: 0.7510 - val_loss: 0.1212 - val_matthews_correlation: 0.4541\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.63786\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0927 - matthews_correlation: 0.6752 - val_loss: 0.0998 - val_matthews_correlation: 0.6456\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.63786 to 0.64556, saving model to weights_2.h5\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0904 - matthews_correlation: 0.6629 - val_loss: 0.1074 - val_matthews_correlation: 0.6429\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.64556\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0681 - matthews_correlation: 0.7830 - val_loss: 0.1234 - val_matthews_correlation: 0.5211\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.64556\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0862 - matthews_correlation: 0.6913 - val_loss: 0.1150 - val_matthews_correlation: 0.6202\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.64556\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0659 - matthews_correlation: 0.7940 - val_loss: 0.1059 - val_matthews_correlation: 0.5603\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.64556\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0662 - matthews_correlation: 0.7755 - val_loss: 0.0957 - val_matthews_correlation: 0.6516\n",
      "\n",
      "Epoch 00021: val_matthews_correlation improved from 0.64556 to 0.65159, saving model to weights_2.h5\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0745 - matthews_correlation: 0.7379 - val_loss: 0.1199 - val_matthews_correlation: 0.5148\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0681 - matthews_correlation: 0.7662 - val_loss: 0.0920 - val_matthews_correlation: 0.6478\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0643 - matthews_correlation: 0.7722 - val_loss: 0.1290 - val_matthews_correlation: 0.6478\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0773 - matthews_correlation: 0.7442 - val_loss: 0.1090 - val_matthews_correlation: 0.4127\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0644 - matthews_correlation: 0.7969 - val_loss: 0.1033 - val_matthews_correlation: 0.5677\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0584 - matthews_correlation: 0.8037 - val_loss: 0.0972 - val_matthews_correlation: 0.5261\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0455 - matthews_correlation: 0.8355 - val_loss: 0.1164 - val_matthews_correlation: 0.5494\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.65159\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0448 - matthews_correlation: 0.8389 - val_loss: 0.1048 - val_matthews_correlation: 0.6646\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.65159 to 0.66461, saving model to weights_2.h5\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0675 - matthews_correlation: 0.7610 - val_loss: 0.1005 - val_matthews_correlation: 0.5796\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0546 - matthews_correlation: 0.8012 - val_loss: 0.1236 - val_matthews_correlation: 0.6128\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0433 - matthews_correlation: 0.8058 - val_loss: 0.1045 - val_matthews_correlation: 0.4957\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0403 - matthews_correlation: 0.8553 - val_loss: 0.1296 - val_matthews_correlation: 0.6545\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0418 - matthews_correlation: 0.8711 - val_loss: 0.1915 - val_matthews_correlation: 0.6130\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0658 - matthews_correlation: 0.7942 - val_loss: 0.1051 - val_matthews_correlation: 0.6548\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0393 - matthews_correlation: 0.8810 - val_loss: 0.1253 - val_matthews_correlation: 0.5482\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0571 - matthews_correlation: 0.8076 - val_loss: 0.1189 - val_matthews_correlation: 0.5610\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0318 - matthews_correlation: 0.9079 - val_loss: 0.1412 - val_matthews_correlation: 0.6442\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0350 - matthews_correlation: 0.8947 - val_loss: 0.1310 - val_matthews_correlation: 0.6552\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0286 - matthews_correlation: 0.9140 - val_loss: 0.1461 - val_matthews_correlation: 0.6059\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0271 - matthews_correlation: 0.9154 - val_loss: 0.1214 - val_matthews_correlation: 0.6387\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.66461\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0301 - matthews_correlation: 0.8976 - val_loss: 0.1453 - val_matthews_correlation: 0.6870\n",
      "\n",
      "Epoch 00042: val_matthews_correlation improved from 0.66461 to 0.68696, saving model to weights_2.h5\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0329 - matthews_correlation: 0.9271 - val_loss: 0.2469 - val_matthews_correlation: 0.4151\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.68696\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0380 - matthews_correlation: 0.8676 - val_loss: 0.1119 - val_matthews_correlation: 0.6903\n",
      "\n",
      "Epoch 00044: val_matthews_correlation improved from 0.68696 to 0.69033, saving model to weights_2.h5\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0171 - matthews_correlation: 0.9695 - val_loss: 0.1902 - val_matthews_correlation: 0.6350\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.69033\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0370 - matthews_correlation: 0.8780 - val_loss: 0.1516 - val_matthews_correlation: 0.6191\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.69033\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0233 - matthews_correlation: 0.9278 - val_loss: 0.1254 - val_matthews_correlation: 0.6967\n",
      "\n",
      "Epoch 00047: val_matthews_correlation improved from 0.69033 to 0.69665, saving model to weights_2.h5\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0223 - matthews_correlation: 0.9150 - val_loss: 0.1714 - val_matthews_correlation: 0.6378\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.69665\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 21s 9ms/step - loss: 0.0210 - matthews_correlation: 0.9215 - val_loss: 0.1312 - val_matthews_correlation: 0.7136\n",
      "\n",
      "Epoch 00049: val_matthews_correlation improved from 0.69665 to 0.71357, saving model to weights_2.h5\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 22s 9ms/step - loss: 0.0215 - matthews_correlation: 0.9298 - val_loss: 0.1860 - val_matthews_correlation: 0.4860\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.71357\n",
      "Beginning fold 4\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 25s 11ms/step - loss: 0.2704 - matthews_correlation: -0.0037 - val_loss: 0.2186 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_3.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.2102 - matthews_correlation: 0.0000e+00 - val_loss: 0.1844 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1555 - matthews_correlation: 0.1835 - val_loss: 0.1416 - val_matthews_correlation: 0.2763\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.27634, saving model to weights_3.h5\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1263 - matthews_correlation: 0.4082 - val_loss: 0.1448 - val_matthews_correlation: 0.5034\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.27634 to 0.50335, saving model to weights_3.h5\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1145 - matthews_correlation: 0.5133 - val_loss: 0.1332 - val_matthews_correlation: 0.5969\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.50335 to 0.59692, saving model to weights_3.h5\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1201 - matthews_correlation: 0.5985 - val_loss: 0.1187 - val_matthews_correlation: 0.6436\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.59692 to 0.64356, saving model to weights_3.h5\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1004 - matthews_correlation: 0.6704 - val_loss: 0.1287 - val_matthews_correlation: 0.6094\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.64356\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1012 - matthews_correlation: 0.6579 - val_loss: 0.1146 - val_matthews_correlation: 0.5743\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.64356\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1021 - matthews_correlation: 0.6230 - val_loss: 0.1152 - val_matthews_correlation: 0.5867\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.64356\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0935 - matthews_correlation: 0.6498 - val_loss: 0.1151 - val_matthews_correlation: 0.5934\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.64356\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0855 - matthews_correlation: 0.6864 - val_loss: 0.1079 - val_matthews_correlation: 0.6002\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.64356\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0842 - matthews_correlation: 0.7077 - val_loss: 0.1159 - val_matthews_correlation: 0.6469\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.64356 to 0.64693, saving model to weights_3.h5\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0833 - matthews_correlation: 0.7077 - val_loss: 0.1117 - val_matthews_correlation: 0.6368\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.64693\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0811 - matthews_correlation: 0.7074 - val_loss: 0.1180 - val_matthews_correlation: 0.6315\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.64693\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0835 - matthews_correlation: 0.7289 - val_loss: 0.1540 - val_matthews_correlation: 0.6161\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.64693\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0855 - matthews_correlation: 0.7401 - val_loss: 0.1083 - val_matthews_correlation: 0.6577\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.64693 to 0.65765, saving model to weights_3.h5\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0778 - matthews_correlation: 0.7205 - val_loss: 0.1663 - val_matthews_correlation: 0.5289\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0913 - matthews_correlation: 0.7041 - val_loss: 0.1143 - val_matthews_correlation: 0.6246\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0760 - matthews_correlation: 0.7786 - val_loss: 0.1151 - val_matthews_correlation: 0.4874\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0781 - matthews_correlation: 0.7194 - val_loss: 0.1317 - val_matthews_correlation: 0.6502\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0745 - matthews_correlation: 0.7183 - val_loss: 0.1054 - val_matthews_correlation: 0.6424\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0579 - matthews_correlation: 0.8145 - val_loss: 0.1320 - val_matthews_correlation: 0.6290\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0883 - matthews_correlation: 0.7256 - val_loss: 0.1155 - val_matthews_correlation: 0.5817\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0616 - matthews_correlation: 0.8194 - val_loss: 0.1156 - val_matthews_correlation: 0.6207\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0606 - matthews_correlation: 0.8150 - val_loss: 0.1252 - val_matthews_correlation: 0.5814\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0549 - matthews_correlation: 0.8464 - val_loss: 0.1174 - val_matthews_correlation: 0.6424\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0438 - matthews_correlation: 0.8848 - val_loss: 0.1298 - val_matthews_correlation: 0.6501\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0504 - matthews_correlation: 0.8553 - val_loss: 0.1279 - val_matthews_correlation: 0.5615\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.65765\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0529 - matthews_correlation: 0.8524 - val_loss: 0.1270 - val_matthews_correlation: 0.6609\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.65765 to 0.66092, saving model to weights_3.h5\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0464 - matthews_correlation: 0.8767 - val_loss: 0.1429 - val_matthews_correlation: 0.5734\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0505 - matthews_correlation: 0.8546 - val_loss: 0.1302 - val_matthews_correlation: 0.5923\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0723 - matthews_correlation: 0.7133 - val_loss: 0.1510 - val_matthews_correlation: 0.5102\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0573 - matthews_correlation: 0.7912 - val_loss: 0.1131 - val_matthews_correlation: 0.6423\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0431 - matthews_correlation: 0.8714 - val_loss: 0.1409 - val_matthews_correlation: 0.5360\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0443 - matthews_correlation: 0.8573 - val_loss: 0.1383 - val_matthews_correlation: 0.6067\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0501 - matthews_correlation: 0.8293 - val_loss: 0.1444 - val_matthews_correlation: 0.6290\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0406 - matthews_correlation: 0.8387 - val_loss: 0.1228 - val_matthews_correlation: 0.6024\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0412 - matthews_correlation: 0.8864 - val_loss: 0.1841 - val_matthews_correlation: 0.4856\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0569 - matthews_correlation: 0.8125 - val_loss: 0.1549 - val_matthews_correlation: 0.4363\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0473 - matthews_correlation: 0.8241 - val_loss: 0.1623 - val_matthews_correlation: 0.4571\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0392 - matthews_correlation: 0.8869 - val_loss: 0.1383 - val_matthews_correlation: 0.5389\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0257 - matthews_correlation: 0.9273 - val_loss: 0.1755 - val_matthews_correlation: 0.5090\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0412 - matthews_correlation: 0.8536 - val_loss: 0.1361 - val_matthews_correlation: 0.6280\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0311 - matthews_correlation: 0.9094 - val_loss: 0.1428 - val_matthews_correlation: 0.5041\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0288 - matthews_correlation: 0.9281 - val_loss: 0.1852 - val_matthews_correlation: 0.5494\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0384 - matthews_correlation: 0.8909 - val_loss: 0.1563 - val_matthews_correlation: 0.5994\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0402 - matthews_correlation: 0.8670 - val_loss: 0.1348 - val_matthews_correlation: 0.5826\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0291 - matthews_correlation: 0.9156 - val_loss: 0.1640 - val_matthews_correlation: 0.4754\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0292 - matthews_correlation: 0.9002 - val_loss: 0.1723 - val_matthews_correlation: 0.4796\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.66092\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0222 - matthews_correlation: 0.9012 - val_loss: 0.1435 - val_matthews_correlation: 0.6056\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.66092\n",
      "Beginning fold 5\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 25s 11ms/step - loss: 0.2846 - matthews_correlation: 0.0029 - val_loss: 0.2193 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_4.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1722 - matthews_correlation: 0.0000e+00 - val_loss: 0.1396 - val_matthews_correlation: 0.4321\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.00000 to 0.43206, saving model to weights_4.h5\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1369 - matthews_correlation: 0.3867 - val_loss: 0.1273 - val_matthews_correlation: 0.5756\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.43206 to 0.57557, saving model to weights_4.h5\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.1064 - matthews_correlation: 0.5785 - val_loss: 0.1333 - val_matthews_correlation: 0.5110\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.57557\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1171 - matthews_correlation: 0.5575 - val_loss: 0.1448 - val_matthews_correlation: 0.6116\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.57557 to 0.61160, saving model to weights_4.h5\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.1092 - matthews_correlation: 0.5395 - val_loss: 0.1355 - val_matthews_correlation: 0.6183\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.61160 to 0.61828, saving model to weights_4.h5\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0934 - matthews_correlation: 0.6457 - val_loss: 0.1241 - val_matthews_correlation: 0.6116\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.61828\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1070 - matthews_correlation: 0.5447 - val_loss: 0.1254 - val_matthews_correlation: 0.6093\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.61828\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.1157 - matthews_correlation: 0.5649 - val_loss: 0.1372 - val_matthews_correlation: 0.4420\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.61828\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0973 - matthews_correlation: 0.6824 - val_loss: 0.1249 - val_matthews_correlation: 0.5960\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.61828\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0823 - matthews_correlation: 0.7052 - val_loss: 0.1270 - val_matthews_correlation: 0.6276\n",
      "\n",
      "Epoch 00011: val_matthews_correlation improved from 0.61828 to 0.62756, saving model to weights_4.h5\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0789 - matthews_correlation: 0.6906 - val_loss: 0.1360 - val_matthews_correlation: 0.5612\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0669 - matthews_correlation: 0.7735 - val_loss: 0.1336 - val_matthews_correlation: 0.5905\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0616 - matthews_correlation: 0.7832 - val_loss: 0.1398 - val_matthews_correlation: 0.6122\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0572 - matthews_correlation: 0.8086 - val_loss: 0.1474 - val_matthews_correlation: 0.5428\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0632 - matthews_correlation: 0.8003 - val_loss: 0.1466 - val_matthews_correlation: 0.5660\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0833 - matthews_correlation: 0.7311 - val_loss: 0.1348 - val_matthews_correlation: 0.5929\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0724 - matthews_correlation: 0.7456 - val_loss: 0.1289 - val_matthews_correlation: 0.5276\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0649 - matthews_correlation: 0.7662 - val_loss: 0.1551 - val_matthews_correlation: 0.5818\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0470 - matthews_correlation: 0.8576 - val_loss: 0.1838 - val_matthews_correlation: 0.6093\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0983 - matthews_correlation: 0.6827 - val_loss: 0.1342 - val_matthews_correlation: 0.5266\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0643 - matthews_correlation: 0.7481 - val_loss: 0.1416 - val_matthews_correlation: 0.5858\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0501 - matthews_correlation: 0.8552 - val_loss: 0.1464 - val_matthews_correlation: 0.5667\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0467 - matthews_correlation: 0.8561 - val_loss: 0.1507 - val_matthews_correlation: 0.6008\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0564 - matthews_correlation: 0.8128 - val_loss: 0.1462 - val_matthews_correlation: 0.5698\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0458 - matthews_correlation: 0.8410 - val_loss: 0.1587 - val_matthews_correlation: 0.5086\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0462 - matthews_correlation: 0.8085 - val_loss: 0.1639 - val_matthews_correlation: 0.6097\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0675 - matthews_correlation: 0.7949 - val_loss: 0.1561 - val_matthews_correlation: 0.5296\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0457 - matthews_correlation: 0.8723 - val_loss: 0.1724 - val_matthews_correlation: 0.5474\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0266 - matthews_correlation: 0.9390 - val_loss: 0.1900 - val_matthews_correlation: 0.5515\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0289 - matthews_correlation: 0.9207 - val_loss: 0.2412 - val_matthews_correlation: 0.5831\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0841 - matthews_correlation: 0.7127 - val_loss: 0.1352 - val_matthews_correlation: 0.5768\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 22s 10ms/step - loss: 0.0571 - matthews_correlation: 0.7513 - val_loss: 0.1522 - val_matthews_correlation: 0.5699\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0453 - matthews_correlation: 0.8802 - val_loss: 0.2242 - val_matthews_correlation: 0.4943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0597 - matthews_correlation: 0.7970 - val_loss: 0.1408 - val_matthews_correlation: 0.5593\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0409 - matthews_correlation: 0.8827 - val_loss: 0.1788 - val_matthews_correlation: 0.5457\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0363 - matthews_correlation: 0.9010 - val_loss: 0.1823 - val_matthews_correlation: 0.5923\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0359 - matthews_correlation: 0.8973 - val_loss: 0.1828 - val_matthews_correlation: 0.5709\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0232 - matthews_correlation: 0.9324 - val_loss: 0.2120 - val_matthews_correlation: 0.5279\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0150 - matthews_correlation: 0.9748 - val_loss: 0.2243 - val_matthews_correlation: 0.5710\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0146 - matthews_correlation: 0.9588 - val_loss: 0.2477 - val_matthews_correlation: 0.4859\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0169 - matthews_correlation: 0.9563 - val_loss: 0.2750 - val_matthews_correlation: 0.5794\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0279 - matthews_correlation: 0.9455 - val_loss: 0.2297 - val_matthews_correlation: 0.5483\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0212 - matthews_correlation: 0.9285 - val_loss: 0.2469 - val_matthews_correlation: 0.6046\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0141 - matthews_correlation: 0.9662 - val_loss: 0.2362 - val_matthews_correlation: 0.5946\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0114 - matthews_correlation: 0.9595 - val_loss: 0.2774 - val_matthews_correlation: 0.5796\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0155 - matthews_correlation: 0.9478 - val_loss: 0.2809 - val_matthews_correlation: 0.5250\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0192 - matthews_correlation: 0.9530 - val_loss: 0.2611 - val_matthews_correlation: 0.5560\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 21s 9ms/step - loss: 0.0206 - matthews_correlation: 0.9448 - val_loss: 0.2706 - val_matthews_correlation: 0.5609\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.62756\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 22s 9ms/step - loss: 0.0347 - matthews_correlation: 0.9020 - val_loss: 0.2335 - val_matthews_correlation: 0.5625\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.62756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2904,), (2904,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "# First, create a set of indexes of the 5 folds\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "preds_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tmp/y_val_tmp_4.npy', y_val)\n",
    "np.save('./tmp/preds_val_tmp_4.npy', preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
   },
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    \n",
    "    #y_pred = K.cast(y_pred, np.float)\n",
    "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = np.sum(y_pos * y_pred_pos)\n",
    "    tn = np.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = np.sum(y_neg * y_pred_pos)\n",
    "    fn = np.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
   },
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        #score = matthews_correlation(y_true, (y_proba > threshold).astype(int))\n",
    "        #score = K.eval(matthews_correlation(y_true, (y_proba > threshold).astype(int)))\n",
    "        #score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        score = matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "6fee7f722ed08bc1453a822a4371ed2d48e08abc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9742.87it/s]\n"
     ]
    }
   ],
   "source": [
    "best_threshold = threshold_search(y_val, preds_val)['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6968210072383328"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_correlation(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 5.14 ms, total: 17.9 ms\n",
      "Wall time: 33.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now load the test data\n",
    "# This first part is the meta data, not the main data, the measurements\n",
    "meta_test = pd.read_csv('../input/metadata_test.csv')\n",
    "df_test = meta_test.set_index(['id_measurement', 'phase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_measurement  phase\n",
       "signal_id                       \n",
       "8712                 2904      0\n",
       "8713                 2904      1\n",
       "8714                 2904      2\n",
       "8715                 2905      0\n",
       "8716                 2905      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test = meta_test.set_index(['signal_id'])\n",
    "meta_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(20337/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20344"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2543*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2543, 0),\n",
       " (2543, 5086, 1),\n",
       " (5086, 7629, 2),\n",
       " (7629, 10172, 3),\n",
       " (10172, 12715, 4),\n",
       " (12715, 15258, 5),\n",
       " (15258, 17801, 6),\n",
       " (17801, 20337, 7)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "847.6666666666666"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2543/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913,\n",
       "            ...\n",
       "            9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682],\n",
       "           dtype='int64', name='id_measurement', length=6779)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.index.levels[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "praq_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(8712, 11255)]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8712</th>\n",
       "      <th>8713</th>\n",
       "      <th>8714</th>\n",
       "      <th>8715</th>\n",
       "      <th>8716</th>\n",
       "      <th>8717</th>\n",
       "      <th>8718</th>\n",
       "      <th>8719</th>\n",
       "      <th>8720</th>\n",
       "      <th>8721</th>\n",
       "      <th>...</th>\n",
       "      <th>11245</th>\n",
       "      <th>11246</th>\n",
       "      <th>11247</th>\n",
       "      <th>11248</th>\n",
       "      <th>11249</th>\n",
       "      <th>11250</th>\n",
       "      <th>11251</th>\n",
       "      <th>11252</th>\n",
       "      <th>11253</th>\n",
       "      <th>11254</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>-20</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-22</td>\n",
       "      <td>-15</td>\n",
       "      <td>-11</td>\n",
       "      <td>20</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>-12</td>\n",
       "      <td>-9</td>\n",
       "      <td>-13</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>20</td>\n",
       "      <td>-15</td>\n",
       "      <td>16</td>\n",
       "      <td>-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>-20</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-21</td>\n",
       "      <td>-16</td>\n",
       "      <td>-12</td>\n",
       "      <td>19</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-22</td>\n",
       "      <td>-10</td>\n",
       "      <td>-13</td>\n",
       "      <td>20</td>\n",
       "      <td>-2</td>\n",
       "      <td>20</td>\n",
       "      <td>-15</td>\n",
       "      <td>16</td>\n",
       "      <td>-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>-21</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>-20</td>\n",
       "      <td>-14</td>\n",
       "      <td>-9</td>\n",
       "      <td>20</td>\n",
       "      <td>-9</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-9</td>\n",
       "      <td>-13</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>-14</td>\n",
       "      <td>17</td>\n",
       "      <td>-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>-21</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>-22</td>\n",
       "      <td>-14</td>\n",
       "      <td>-8</td>\n",
       "      <td>22</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>-15</td>\n",
       "      <td>-9</td>\n",
       "      <td>-13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>-14</td>\n",
       "      <td>17</td>\n",
       "      <td>-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>-20</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>-22</td>\n",
       "      <td>-12</td>\n",
       "      <td>-6</td>\n",
       "      <td>24</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-20</td>\n",
       "      <td>-8</td>\n",
       "      <td>-12</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>22</td>\n",
       "      <td>-13</td>\n",
       "      <td>16</td>\n",
       "      <td>-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2543 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   8712  8713  8714  8715  8716  8717  8718  8719  8720  8721  ...    11245  \\\n",
       "0    15   -20     3    11     1   -22   -15   -11    20    -7  ...       -3   \n",
       "1    15   -20     3    11     1   -21   -16   -12    19    -7  ...       -5   \n",
       "2    14   -21     2    14     2   -20   -14    -9    20    -9  ...       -2   \n",
       "3    14   -21     2    13     1   -22   -14    -8    22    -7  ...       -3   \n",
       "4    15   -20     3    11     2   -22   -12    -6    24    -8  ...       -4   \n",
       "\n",
       "   11246  11247  11248  11249  11250  11251  11252  11253  11254  \n",
       "0    -12     -9    -13     19     -1     20    -15     16    -18  \n",
       "1    -22    -10    -13     20     -2     20    -15     16    -19  \n",
       "2    -12     -9    -13     20     -1     21    -14     17    -18  \n",
       "3    -15     -9    -13     20      0     22    -14     17    -17  \n",
       "4    -20     -8    -12     20     -1     22    -13     16    -18  \n",
       "\n",
       "[5 rows x 2543 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "praq_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11255"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2543+8712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([8837, 8838, 8839, 8840, 8841, 8842, 8843, 8844, 8845, 8846,\n",
       "            ...\n",
       "            9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682],\n",
       "           dtype='int64', name='id_measurement', length=846)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.index.levels[0].unique()[int(17801/3):int(20337/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9681</th>\n",
       "      <th>1</th>\n",
       "      <td>29044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">9682</th>\n",
       "      <th>0</th>\n",
       "      <td>29046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id\n",
       "id_measurement phase           \n",
       "9681           1          29044\n",
       "               2          29045\n",
       "9682           0          29046\n",
       "               1          29047\n",
       "               2          29048"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2543, 0),\n",
       " (2543, 5086, 1),\n",
       " (5086, 7629, 2),\n",
       " (7629, 10172, 3),\n",
       " (10172, 12715, 4),\n",
       " (12715, 15258, 5),\n",
       " (15258, 17801, 6),\n",
       " (17801, 20337, 7)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "signal_id    11252\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[3750].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913,\n",
       "            ...\n",
       "            3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750],\n",
       "           dtype='int64', name='id_measurement', length=847)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.index.levels[0].unique()[int(0/3):int(2543/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760,\n",
       "            ...\n",
       "            4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598],\n",
       "           dtype='int64', name='id_measurement', length=848)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.index.levels[0].unique()[int(2543/3):int(5086/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337,\n",
       "            ...\n",
       "            3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751],\n",
       "           dtype='int64', name='id_measurement', length=424)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.index.levels[0].unique()[int(1272/3):int(2544/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9984"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[3328].loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9983'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df_test.loc[3327].loc[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9984"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1272+8712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data_test(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start+8712, end+8712)]).to_pandas()\n",
    "    X = []\n",
    "\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_test.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id = df_test.loc[id_measurement].loc[phase][0]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            \n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtest(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X = prep_data_test(start, end)\n",
    "    return idx, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "num_cores = 16 \n",
    "#def load_all():\n",
    "total_size = len(meta_test)\n",
    "chunk_size = np.ceil(total_size/num_cores)\n",
    "#train_size = len(df_train)\n",
    "\n",
    "for i in range(16):\n",
    "    if i != 15:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(start_idx + chunk_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "    else:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(total_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 424/424 [15:28<00:00,  2.43s/it] \n",
      "100%|██████████| 424/424 [15:31<00:00,  2.17s/it]\n",
      "100%|██████████| 424/424 [17:14<00:00,  1.06s/it]\n",
      "100%|██████████| 424/424 [17:18<00:00,  1.02s/it]\n",
      "100%|██████████| 424/424 [17:32<00:00,  2.86s/it]\n",
      "100%|██████████| 424/424 [17:31<00:00,  1.15it/s]\n",
      "100%|██████████| 424/424 [17:45<00:00,  1.34it/s]\n",
      "100%|██████████| 424/424 [18:13<00:00,  1.23s/it]\n",
      "100%|██████████| 424/424 [15:23<00:00,  3.49s/it] \n",
      "100%|██████████| 424/424 [16:28<00:00,  4.98s/it]\n",
      "100%|██████████| 419/419 [16:34<00:00,  2.02s/it]\n",
      "100%|██████████| 424/424 [16:48<00:00,  1.80s/it]\n",
      " 88%|████████▊ | 374/424 [16:28<00:50,  1.00s/it]\n",
      "100%|██████████| 424/424 [17:20<00:00,  1.37it/s]\n",
      "100%|██████████| 424/424 [17:22<00:00,  1.07s/it]\n",
      "100%|██████████| 424/424 [17:50<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "pool = Pool()\n",
    "results_1 = pool.map(process_subtest, all_chunks[0:8])    \n",
    "results_1 = sorted(results_1, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = pool.map(process_subtest, all_chunks[8:16])    \n",
    "results_2 = sorted(results_2, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_1 + results_2\n",
    "X_test = np.concatenate([item[1] for item in results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_2.npy\",X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "num_cores = 8 \n",
    "#def load_all():\n",
    "total_size = len(meta_test)\n",
    "chunk_size = np.ceil(total_size/num_cores)\n",
    "\n",
    "for i in range(8):\n",
    "    start_idx = int(i * chunk_size)\n",
    "    end_idx = int(start_idx + chunk_size)\n",
    "    chunk = (start_idx, end_idx, i)\n",
    "    all_chunks.append(chunk)\n",
    "\n",
    "pool = Pool()\n",
    "results = pool.map(process_subtrain, all_chunks)    \n",
    "results = sorted(results, key=lambda tup: tup[0])\n",
    "\n",
    "X = np.concatenate([item[1] for item in results], axis=0)\n",
    "y = np.concatenate([item[2] for item in results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8712"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712 10 20337 2033 7 20337\n"
     ]
    }
   ],
   "source": [
    "first_sig = meta_test.index[0]\n",
    "n_parts = 10\n",
    "max_line = len(meta_test)\n",
    "part_size = int(max_line / n_parts)\n",
    "last_part = max_line % n_parts\n",
    "print(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8712, 10745], [10745, 12778], [12778, 14811], [14811, 16844], [16844, 18877], [18877, 20910], [20910, 22943], [22943, 24976], [24976, 27009], [27009, 29042], [29042, 29049]]\n"
     ]
    }
   ],
   "source": [
    "# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\n",
    "start_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\n",
    "start_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\n",
    "print(start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f8e94387f625bff0a9a6289e1ee038908bc5856"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# First we daclarete a series of parameters to initiate the loading of the main data\n",
    "# it is too large, it is impossible to load in one time, so we are doing it in dividing in 10 parts\n",
    "first_sig = meta_test.index[0]\n",
    "n_parts = 10\n",
    "max_line = len(meta_test)\n",
    "part_size = int(max_line / n_parts)\n",
    "last_part = max_line % n_parts\n",
    "print(first_sig, n_parts, max_line, part_size, last_part, n_parts * part_size + last_part)\n",
    "# Here we create a list of lists with start index and end index for each of the 10 parts and one for the last partial part\n",
    "start_end = [[x, x+part_size] for x in range(first_sig, max_line + first_sig, part_size)]\n",
    "start_end = start_end[:-1] + [[start_end[-1][0], start_end[-1][0] + last_part]]\n",
    "print(start_end)\n",
    "X_test = []\n",
    "# now, very like we did above with the train data, we convert the test data part by part\n",
    "# transforming the 3 phases 800000 measurement in matrix (160,57)\n",
    "for start, end in start_end:\n",
    "    subset_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    for i in tqdm(subset_test.columns):\n",
    "        id_measurement, phase = meta_test.loc[int(i)]\n",
    "        subset_test_col = subset_test[i]\n",
    "        subset_trans = transform_ts(subset_test_col)\n",
    "        X_test.append([i, id_measurement, phase, subset_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af9aa6b2b8f8a2beda1a02ff998e3072fcad8d06"
   },
   "outputs": [],
   "source": [
    "X_test_input = np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])\n",
    "np.save(\"X_test.npy\",X_test_input)\n",
    "X_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_input = np.load('./X_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(len(submission))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Beginning fold 2\n",
      "Beginning fold 3\n",
      "Beginning fold 4\n",
      "Beginning fold 5\n"
     ]
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 17s 3ms/step\n",
      "6779/6779 [==============================] - 16s 2ms/step\n",
      "6779/6779 [==============================] - 16s 2ms/step\n",
      "6779/6779 [==============================] - 16s 2ms/step\n",
      "6779/6779 [==============================] - 16s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_test = []\n",
    "for i in range(N_SPLITS):\n",
    "    model.load_weights('weights_{}.h5'.format(i))\n",
    "    pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "    pred_3 = []\n",
    "    for pred_scalar in pred:\n",
    "        for i in range(3):\n",
    "            pred_3.append(pred_scalar)\n",
    "    preds_test.append(pred_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "9f76c471eaf983707d446c5081ab3d50c4e40ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20337,)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = (np.squeeze(np.mean(preds_test, axis=0)) > best_threshold).astype(np.int)\n",
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "b35723f85d494b4b6ec630dd7c79135a110a4062"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'] = preds_test\n",
    "submission.to_csv('../output/submission_18.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7600d0093a9880003240ef9ce0a1f1303e4d982"
   },
   "outputs": [],
   "source": [
    "lstm_preds = np.squeeze(np.mean(preds_test, axis=0))\n",
    "np.save(\"./lstm_preds.npy\", lstm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
