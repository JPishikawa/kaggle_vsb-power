{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e154a47bf09b8770980486e87786317a1b3038e1"
   },
   "source": [
    "### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.\n",
    "### If any part is not clear, please comment.  \n",
    "### Please upvote if it was helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwademo123/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
    "\n",
    "from scipy.signal import chirp, find_peaks, peak_widths\n",
    "import pywt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e6379386e44afc69bee8895a52da22199e888fb"
   },
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "sample_size = 800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "eda7ea366117d1ce8e5fce69e5bba333821d8b48"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        x = K.concatenate([weighted_input, x], axis=2)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], self.features_dim*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('../input/metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "26df6c7fbfecd537404866faec13d1238ae3ebc6"
   },
   "outputs": [],
   "source": [
    "# in other notebook I have extracted the min and max values from the train data, the measurements\n",
    "max_num = 127\n",
    "min_num = -128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "7b0717b14bcfcba1f48d33c8161ae51c778687af"
   },
   "outputs": [],
   "source": [
    "# This function standardize the data from (-128 to 127) to (-1 to 1)\n",
    "# Theoretically it helps in the NN Model training, but I didn't tested without it\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maddest(d, axis=None):\n",
    "    \"\"\"\n",
    "    Mean Absolute Deviation\n",
    "    \"\"\"\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal( x, wavelet='db4', level=1):\n",
    "    \"\"\"\n",
    "    1. Adapted from waveletSmooth function found here:\n",
    "    http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "    2. Threshold equation and using hard mode in threshold as mentioned\n",
    "    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "    http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\", level=level)\n",
    "    \n",
    "    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "\n",
    "    # Calculte the univeral threshold\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec( coeff[1:], wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal_2( x, wavelet='db4', level=1):\n",
    "    \"\"\"\n",
    "    1. Adapted from waveletSmooth function found here:\n",
    "    http://connor-johnson.com/2016/01/24/using-pywavelets-to-remove-high-frequency-noise/\n",
    "    2. Threshold equation and using hard mode in threshold as mentioned\n",
    "    in section '3.2 denoising based on optimized singular values' from paper by Tomas Vantuch:\n",
    "    http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Decompose to get the wavelet coefficients\n",
    "    coeff = pywt.wavedec( x, wavelet, mode=\"per\", level=level)\n",
    "    \n",
    "    # Calculate sigma for threshold as defined in http://dspace.vsb.cz/bitstream/handle/10084/133114/VAN431_FEI_P1807_1801V001_2018.pdf\n",
    "    # As noted by @harshit92 MAD referred to in the paper is Mean Absolute Deviation not Median Absolute Deviation\n",
    "    sigma = (1/0.6745) * maddest( coeff[-level] )\n",
    "\n",
    "    # Calculte the univeral threshold\n",
    "    uthresh = sigma * np.sqrt( 2*np.log( len( x ) ) )\n",
    "    coeff[1:] = ( pywt.threshold( i, value=uthresh, mode='hard' ) for i in coeff[1:] )\n",
    "    \n",
    "    # Reconstruct the signal using the thresholded coefficients\n",
    "    return pywt.waverec( coeff[0:], wavelet, mode='per' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corona(x_dn, maxDistance=10, maxHeightRatio=0.25, maxTicksRemoval=500):\n",
    "    index = pd.Series(x_dn).loc[np.abs(x_dn)>0].index\n",
    "    corona_idx = []\n",
    "    for idx in index:\n",
    "        for i in range(1,maxDistance+1):\n",
    "            if idx+i < pd.Series(x_dn).shape[0]:\n",
    "                if x_dn[idx+i]/(x_dn[idx]+1e-04)<-maxHeightRatio:\n",
    "                    x_dn[idx:idx+maxTicksRemoval] = 0\n",
    "                    corona_idx.append(idx)\n",
    "    return x_dn, corona_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    #ts_wave = denoise_signal_2(ts_std, wavelet='haar', level=1)\n",
    "    #ts_rm, corona_idx = remove_corona(ts_wave)\n",
    "    \n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        #ts_wave_range = ts_wave[i:i + bucket_size]\n",
    "        #ts_rm_range = pd.Series(ts_rm_range)\n",
    "        \n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        \n",
    "        #mean_wave = ts_wave_range.mean()\n",
    "        #numpeaks = np.sum(ts_range!=0)/len(ts_range)\n",
    "        #numpospeaks = ts_rm_range[ts_rm_range>0].count()\n",
    "        #numpospeaks = np.sum(ts_range>0)/len(ts_range)\n",
    "        #numnegpeaks = ts_rm_range[ts_rm_range<0].count()\n",
    "        #numnegpeaks = np.sum(ts_range<0)/len(ts_range)\n",
    "        #numpeaks = ts_rm_range[ts_rm_range!=0].count()\n",
    "        #numpeaks = np.sum(ts_rm_range!=0)/len(ts_rm_range)\n",
    "        #numpospeaks = ts_rm_range[ts_rm_range>0].count()\n",
    "        #numpospeaks = np.sum(ts_rm_range>0)/len(ts_rm_range)\n",
    "        #numnegpeaks = ts_rm_range[ts_rm_range<0].count()\n",
    "        #numnegpeaks = np.sum(ts_rm_range<0)/len(ts_rm_range)\n",
    "\n",
    "        #meanamp = np.mean(ts_rm_range)\n",
    "        #meanamppos = np.mean(ts_rm_range[ts_rm_range>0])\n",
    "        #meanampneg = np.mean(ts_rm_range[ts_rm_range<0])\n",
    "\n",
    "        #maxamp = np.max(ts_rm_range)\n",
    "        #minamp = np.min(ts_rm_range)\n",
    "        \n",
    "        #peaks, _ = find_peaks(ts_rm_range)\n",
    "        #results_full = peak_widths(ts_rm_range, peaks, rel_height=1)\n",
    "        \n",
    "        #if len(results_full[0])==0:\n",
    "         #   maxwidth = 0\n",
    "         #   minwidth = 0\n",
    "        #else:\n",
    "         #   maxwidth = np.max(results_full[0])/len(ts_rm_range)\n",
    "         #   minwidth = np.min(results_full[0])/len(ts_rm_range)\n",
    "        \n",
    "        \n",
    "        \n",
    "        feat_array = np.asarray([mean, std, std_top, std_bot, max_range], dtype=np.float32)\n",
    "        #feat_array_2 = np.asarray([numpeaks, numpospeaks, numnegpeaks\n",
    "        #                           , meanamp, meanamppos, meanampneg,\n",
    "        #                         maxamp, minamp, maxwidth, minwidth\n",
    "        #                          ])\n",
    "        \n",
    "        new_ts.append(np.concatenate([feat_array\n",
    "                                      , percentil_calc, relative_percentile]))\n",
    "        \n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #scaler = StandardScaler()\n",
    "    #new_ts = np.asarray(new_ts)\n",
    "    #new_ts[np.isnan(new_ts)] = 0\n",
    "    \n",
    "    #new_ts = scaler.fit_transform(new_ts)\n",
    "        \n",
    "    return new_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "c6137bbbe75c3a1509a5f98e08805dbbd492aa37"
   },
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "def transform_ts_sum(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    #ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    #ts_wave = denoise_signal(ts_std, wavelet='haar', level=1)\n",
    "    #ts_rm, corona_idx = remove_corona(ts_wave)\n",
    "    \n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, 400000, 2500):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts[i:i + 2500]\n",
    "        #ts_rm_range = ts_rm[int(i/2):int(i/2) + int(bucket_size/2)]\n",
    "        #ts_rm_range = pd.Series(ts_rm_range)\n",
    "        \n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        \n",
    "        #numpeaks = ts_rm_range[ts_rm_range!=0].count()\n",
    "        \n",
    "        numpeaks = np.sum(ts_range!=0)/len(ts_range)\n",
    "        #numpospeaks = ts_rm_range[ts_rm_range>0].count()\n",
    "        numpospeaks = np.sum(ts_range>0)/len(ts_range)\n",
    "        #numnegpeaks = ts_rm_range[ts_rm_range<0].count()\n",
    "        numnegpeaks = np.sum(ts_range<0)/len(ts_range)\n",
    "\n",
    "        #meanamp = np.mean(ts_rm_range)\n",
    "        meanamppos = np.mean(ts_range[ts_range>0])\n",
    "        meanampneg = np.mean(ts_range[ts_range<0])\n",
    "\n",
    "        #maxamp = np.max(ts_rm_range)\n",
    "        #minamp = np.min(ts_rm_range)\n",
    "        \n",
    "        #peaks, _ = find_peaks(ts_range)\n",
    "        #results_full = peak_widths(ts_range, peaks, rel_height=1)\n",
    "        \n",
    "        #if len(results_full[0])==0:\n",
    "        #    maxwidth = 0\n",
    "        #    minwidth = 0\n",
    "        #else:\n",
    "        #    maxwidth = np.max(results_full[0])/len(ts_range)\n",
    "        #    minwidth = np.min(results_full[0])/len(ts_range)\n",
    "        \n",
    "        \n",
    "        \n",
    "        feat_array = np.asarray([mean, std, std_top, std_bot, max_range])\n",
    "        feat_array_2 = np.asarray([numpeaks, numpospeaks, numnegpeaks\n",
    "                                   #, meanamppos, meanampneg,\n",
    "                                 #maxwidth, minwidth\n",
    "                                  ])\n",
    "        \n",
    "        new_ts.append(np.concatenate([feat_array, feat_array_2, percentil_calc, relative_percentile]))\n",
    "        \n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #scaler = StandardScaler()\n",
    "    #new_ts = np.asarray(new_ts)\n",
    "    #new_ts[np.isnan(new_ts)] = 0\n",
    "    \n",
    "    #new_ts = scaler.fit_transform(new_ts)\n",
    "        \n",
    "    return new_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_train = pq.read_pandas('../input/train.parquet', columns=[str(i) for i in range(start, end)]).to_pandas()\n",
    "    X = []\n",
    "    y = []\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)                \n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(np.asarray(praq_train[str(signal_id)], dtype=np.float32)))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtrain(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X, y = prep_data(start, end)\n",
    "    return idx, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "52dc826ab9ee1dd56c9fb29bd5c1b2d26b5928bf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 363/363 [04:43<00:00,  1.27it/s]\n",
      "100%|██████████| 363/363 [04:45<00:00,  1.33it/s]\n",
      "100%|██████████| 363/363 [04:47<00:00,  1.47it/s]\n",
      "100%|██████████| 363/363 [04:47<00:00,  1.33it/s]\n",
      "100%|██████████| 363/363 [04:47<00:00,  1.48it/s]\n",
      "100%|██████████| 363/363 [04:47<00:00,  1.64it/s]\n",
      "100%|██████████| 363/363 [04:47<00:00,  2.24it/s]\n",
      "100%|██████████| 363/363 [04:48<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# this code is very simple, divide the total size of the df_train into two sets and process it\n",
    "#X = []\n",
    "#y = []\n",
    "all_chunks = []\n",
    "\n",
    "num_cores = 8 \n",
    "#def load_all():\n",
    "total_size = len(df_train)\n",
    "chunk_size = total_size/num_cores\n",
    "\n",
    "for i in range(8):\n",
    "    start_idx = int(i * chunk_size)\n",
    "    end_idx = int(start_idx + chunk_size)\n",
    "    chunk = (start_idx, end_idx, i)\n",
    "    all_chunks.append(chunk)\n",
    "\n",
    "pool = Pool()\n",
    "results = pool.map(process_subtrain, all_chunks)    \n",
    "results = sorted(results, key=lambda tup: tup[0])\n",
    "\n",
    "X = np.concatenate([item[1] for item in results], axis=0)\n",
    "y = np.concatenate([item[2] for item in results], axis=0)\n",
    "\n",
    "#load_all()\n",
    "\n",
    "#X = np.asarray(X)\n",
    "#y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 160, 57) (2904,)\n"
     ]
    }
   ],
   "source": [
    "# The X shape here is very important. It is also important undertand a little how a LSTM works\n",
    "# X.shape[0] is the number of id_measuremts contained in train data\n",
    "# X.shape[1] is the number of chunks resultant of the transformation, each of this date enters in the LSTM serialized\n",
    "# This way the LSTM can understand the position of a data relative with other and activate a signal that needs\n",
    "# a serie of inputs in a specifc order.\n",
    "# X.shape[3] is the number of features multiplied by the number of phases (3)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 int32\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.reshape(2904, 160, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 -1.2950604259967804\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X), np.min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "51ad0e25b00536de6170168499923d82ae1d735f"
   },
   "outputs": [],
   "source": [
    "# save data into file, a numpy specific format\n",
    "np.save(\"X.npy\",X)\n",
    "np.save(\"y.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"./X.npy\")\n",
    "y = np.load(\"./y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 (2904, 160, 57)\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtype = 'float32'\n",
    "y.dtype = 'int32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 int32 (2904, 160, 114) (5808,)\n"
     ]
    }
   ],
   "source": [
    "print(X.dtype, y.dtype, X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(Layer):\n",
    "  \n",
    "    def __init__(self, depth:int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.depth = depth\n",
    "        self.q_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.k_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.v_dense_layer = Dense(depth, use_bias=False)\n",
    "        self.output_dense_layer = Dense(depth, use_bias=False)\n",
    "    \n",
    "    def call(self, inp):\n",
    "        q = self.q_dense_layer(inp)  # [batch_size, q_length, depth]\n",
    "        q *= self.depth ** -0.5\n",
    "        print(q.shape)\n",
    "        \n",
    "        k = self.k_dense_layer(inp)  # [batch_size, m_length, depth]\n",
    "        v = self.v_dense_layer(inp)\n",
    "\n",
    "        logit = tf.matmul(q, k, transpose_b=True)\n",
    "        print(logit.shape)\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(logit, name='attention_weight')\n",
    "        \n",
    "        attention_output = tf.matmul(attention_weight, v)  # [batch_size, q_length, depth]\n",
    "        print(attention_output.shape)\n",
    "        \n",
    "        x = self.output_dense_layer(attention_output) + q\n",
    "        print(x.shape)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if len(input_shape)==3:\n",
    "            return input_shape[0], input_shape[1], self.depth\n",
    "        if len(input_shape)==4:\n",
    "            return input_shape[0], input_shape[1], input_shape[2], self.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2]))\n",
    "    \n",
    "    depth = 57\n",
    "    \n",
    "    x_1 = SimpleAttention(depth)(inp)\n",
    "    x_2 = Dense(depth)(x_1)\n",
    "    x = Average()([x_1, x_2])\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    x = SimpleAttention(depth)(x)\n",
    "    x_1 = Dense(depth)(x)\n",
    "    x = Average()([x, x_1])\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    x = SimpleAttention(depth)(x)\n",
    "    x_1 = Dense(depth)(x)\n",
    "    x = Average()([x, x_1])\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    x = SimpleAttention(depth)(x)\n",
    "    x_1 = Dense(depth)(x)\n",
    "    x = Average()([x, x_1])\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    #x = Lambda(lambda x: K.sum(x, axis=2))(x)\n",
    "    \n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    #x = SimpleAttention(15)(x)\n",
    "    #x_1 = Dense(15)(x)\n",
    "    #x = Average()([x, x_1])\n",
    "    #x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    #x = SimpleAttention(15)(x)\n",
    "    #x_1 = Dense(15)(x)\n",
    "    #x = Average()([x, x_1])\n",
    "    #x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    x = SimpleAttention(depth)(x)\n",
    "    x_1 = Dense(depth)(x)\n",
    "    x = Average()([x, x_1])\n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    \n",
    "    #print(x.shape)\n",
    "    \n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    \n",
    "    #x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    \n",
    "    #x = Bidirectional(CuDNNLSTM(30, return_sequences=True))(x)\n",
    "    #x = Bidirectional(CuDNNLSTM(30, return_sequences=True))(x)\n",
    "    \n",
    "    #x = Attention(input_shape[1])(x)\n",
    "    x = Lambda(lambda x: K.sum(x, axis=1))(x)\n",
    "    #x = Dense(60)(x)\n",
    "    \n",
    "    #x = K.sum(x, axis=1)\n",
    "    #x = Dense(60)(x)\n",
    "    #x = K.reshape(x, (-1, 160, 60))\n",
    "    \n",
    "    print(x.shape)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 160, 57)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_6 (SimpleAtten (None, 160, 57)      0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 160, 57)      3306        simple_attention_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_6 (Average)             (None, 160, 57)      0           simple_attention_6[0][0]         \n",
      "                                                                 dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 160, 57)      0           average_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_7 (SimpleAtten (None, 160, 57)      0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 160, 57)      3306        simple_attention_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_7 (Average)             (None, 160, 57)      0           simple_attention_7[0][0]         \n",
      "                                                                 dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 160, 57)      0           average_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_8 (SimpleAtten (None, 160, 57)      0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 160, 57)      3306        simple_attention_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_8 (Average)             (None, 160, 57)      0           simple_attention_8[0][0]         \n",
      "                                                                 dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 160, 57)      0           average_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_9 (SimpleAtten (None, 160, 57)      0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 160, 57)      3306        simple_attention_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "average_9 (Average)             (None, 160, 57)      0           simple_attention_9[0][0]         \n",
      "                                                                 dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 160, 57)      0           average_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "simple_attention_10 (SimpleAtte (None, 160, 57)      0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 160, 57)      3306        simple_attention_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_10 (Average)            (None, 160, 57)      0           simple_attention_10[0][0]        \n",
      "                                                                 dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 57)           0           average_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 57)           0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 1)            58          leaky_re_lu_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 16,588\n",
      "Trainable params: 16,588\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_lstm(X.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "8d6f4ca319c383b1b4f671a37c5a324136e7a466",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "Train on 2322 samples, validate on 582 samples\n",
      "Epoch 1/50\n",
      "2322/2322 [==============================] - 9s 4ms/step - loss: 0.3123 - matthews_correlation: 0.0000e+00 - val_loss: 0.2294 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_0.h5\n",
      "Epoch 2/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.2099 - matthews_correlation: 0.0409 - val_loss: 0.2031 - val_matthews_correlation: 0.2300\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.00000 to 0.23003, saving model to weights_0.h5\n",
      "Epoch 3/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1955 - matthews_correlation: 0.1173 - val_loss: 0.1941 - val_matthews_correlation: 0.2300\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.23003\n",
      "Epoch 4/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1874 - matthews_correlation: 0.2445 - val_loss: 0.1747 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.23003\n",
      "Epoch 5/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1686 - matthews_correlation: 0.0807 - val_loss: 0.1576 - val_matthews_correlation: 0.2300\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.23003\n",
      "Epoch 6/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1571 - matthews_correlation: 0.2285 - val_loss: 0.1327 - val_matthews_correlation: 0.3724\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.23003 to 0.37235, saving model to weights_0.h5\n",
      "Epoch 7/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1406 - matthews_correlation: 0.2201 - val_loss: 0.1164 - val_matthews_correlation: 0.4791\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.37235 to 0.47906, saving model to weights_0.h5\n",
      "Epoch 8/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1312 - matthews_correlation: 0.5062 - val_loss: 0.1179 - val_matthews_correlation: 0.3329\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.47906\n",
      "Epoch 9/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1282 - matthews_correlation: 0.5240 - val_loss: 0.1045 - val_matthews_correlation: 0.6226\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.47906 to 0.62263, saving model to weights_0.h5\n",
      "Epoch 10/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1270 - matthews_correlation: 0.4728 - val_loss: 0.1154 - val_matthews_correlation: 0.3329\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.62263\n",
      "Epoch 11/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1228 - matthews_correlation: 0.5104 - val_loss: 0.1023 - val_matthews_correlation: 0.4307\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.62263\n",
      "Epoch 12/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1233 - matthews_correlation: 0.5179 - val_loss: 0.0948 - val_matthews_correlation: 0.6299\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.62263 to 0.62994, saving model to weights_0.h5\n",
      "Epoch 13/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1292 - matthews_correlation: 0.4555 - val_loss: 0.1029 - val_matthews_correlation: 0.5301\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.62994\n",
      "Epoch 14/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1186 - matthews_correlation: 0.5341 - val_loss: 0.0958 - val_matthews_correlation: 0.6240\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.62994\n",
      "Epoch 15/50\n",
      "2322/2322 [==============================] - 6s 3ms/step - loss: 0.1186 - matthews_correlation: 0.5148 - val_loss: 0.0929 - val_matthews_correlation: 0.5199\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.62994\n",
      "Epoch 16/50\n",
      "2322/2322 [==============================] - 6s 2ms/step - loss: 0.1110 - matthews_correlation: 0.4651 - val_loss: 0.1022 - val_matthews_correlation: 0.4704\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.62994\n",
      "Epoch 17/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1160 - matthews_correlation: 0.5257 - val_loss: 0.0959 - val_matthews_correlation: 0.7114\n",
      "\n",
      "Epoch 00017: val_matthews_correlation improved from 0.62994 to 0.71138, saving model to weights_0.h5\n",
      "Epoch 18/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1145 - matthews_correlation: 0.5702 - val_loss: 0.0878 - val_matthews_correlation: 0.6507\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 19/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1082 - matthews_correlation: 0.5689 - val_loss: 0.0899 - val_matthews_correlation: 0.5457\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 20/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1088 - matthews_correlation: 0.5044 - val_loss: 0.0943 - val_matthews_correlation: 0.4958\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 21/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1050 - matthews_correlation: 0.5742 - val_loss: 0.0881 - val_matthews_correlation: 0.6736\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 22/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1042 - matthews_correlation: 0.5535 - val_loss: 0.0817 - val_matthews_correlation: 0.6419\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 23/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1048 - matthews_correlation: 0.5778 - val_loss: 0.0924 - val_matthews_correlation: 0.4703\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 24/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1070 - matthews_correlation: 0.6030 - val_loss: 0.0898 - val_matthews_correlation: 0.5245\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.71138\n",
      "Epoch 25/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1099 - matthews_correlation: 0.5217 - val_loss: 0.0851 - val_matthews_correlation: 0.7284\n",
      "\n",
      "Epoch 00025: val_matthews_correlation improved from 0.71138 to 0.72835, saving model to weights_0.h5\n",
      "Epoch 26/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1009 - matthews_correlation: 0.5887 - val_loss: 0.0796 - val_matthews_correlation: 0.6963\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.72835\n",
      "Epoch 27/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1013 - matthews_correlation: 0.5417 - val_loss: 0.0830 - val_matthews_correlation: 0.7185\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.72835\n",
      "Epoch 28/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1088 - matthews_correlation: 0.5449 - val_loss: 0.0926 - val_matthews_correlation: 0.4958\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.72835\n",
      "Epoch 29/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1117 - matthews_correlation: 0.5744 - val_loss: 0.0831 - val_matthews_correlation: 0.7336\n",
      "\n",
      "Epoch 00029: val_matthews_correlation improved from 0.72835 to 0.73363, saving model to weights_0.h5\n",
      "Epoch 30/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1027 - matthews_correlation: 0.5371 - val_loss: 0.0876 - val_matthews_correlation: 0.4372\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 31/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1098 - matthews_correlation: 0.5067 - val_loss: 0.0947 - val_matthews_correlation: 0.7071\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 32/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1064 - matthews_correlation: 0.5600 - val_loss: 0.0802 - val_matthews_correlation: 0.6853\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1092 - matthews_correlation: 0.5292 - val_loss: 0.0839 - val_matthews_correlation: 0.5076\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 34/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1032 - matthews_correlation: 0.5980 - val_loss: 0.0844 - val_matthews_correlation: 0.4807\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 35/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0991 - matthews_correlation: 0.5641 - val_loss: 0.0812 - val_matthews_correlation: 0.5215\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 36/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1019 - matthews_correlation: 0.6213 - val_loss: 0.0814 - val_matthews_correlation: 0.5408\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 37/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0972 - matthews_correlation: 0.5963 - val_loss: 0.0868 - val_matthews_correlation: 0.4960\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 38/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0976 - matthews_correlation: 0.6327 - val_loss: 0.0781 - val_matthews_correlation: 0.5633\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 39/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0985 - matthews_correlation: 0.6007 - val_loss: 0.0800 - val_matthews_correlation: 0.4664\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 40/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0998 - matthews_correlation: 0.6220 - val_loss: 0.0793 - val_matthews_correlation: 0.6853\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 41/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0997 - matthews_correlation: 0.5945 - val_loss: 0.0862 - val_matthews_correlation: 0.4836\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 42/50\n",
      "2322/2322 [==============================] - 6s 2ms/step - loss: 0.1077 - matthews_correlation: 0.5518 - val_loss: 0.0883 - val_matthews_correlation: 0.7308\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 43/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.1139 - matthews_correlation: 0.6215 - val_loss: 0.0839 - val_matthews_correlation: 0.6963\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 44/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0968 - matthews_correlation: 0.5661 - val_loss: 0.0767 - val_matthews_correlation: 0.7240\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.73363\n",
      "Epoch 45/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0939 - matthews_correlation: 0.6204 - val_loss: 0.0768 - val_matthews_correlation: 0.7402\n",
      "\n",
      "Epoch 00045: val_matthews_correlation improved from 0.73363 to 0.74019, saving model to weights_0.h5\n",
      "Epoch 46/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0976 - matthews_correlation: 0.5924 - val_loss: 0.0786 - val_matthews_correlation: 0.7594\n",
      "\n",
      "Epoch 00046: val_matthews_correlation improved from 0.74019 to 0.75935, saving model to weights_0.h5\n",
      "Epoch 47/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0939 - matthews_correlation: 0.6033 - val_loss: 0.0751 - val_matthews_correlation: 0.7303\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.75935\n",
      "Epoch 48/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0940 - matthews_correlation: 0.5991 - val_loss: 0.0809 - val_matthews_correlation: 0.4892\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.75935\n",
      "Epoch 49/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0974 - matthews_correlation: 0.6182 - val_loss: 0.0789 - val_matthews_correlation: 0.6515\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.75935\n",
      "Epoch 50/50\n",
      "2322/2322 [==============================] - 5s 2ms/step - loss: 0.0974 - matthews_correlation: 0.6473 - val_loss: 0.0977 - val_matthews_correlation: 0.4960\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.75935\n",
      "Beginning fold 2\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 7s 3ms/step - loss: 0.2943 - matthews_correlation: 0.0000e+00 - val_loss: 0.2266 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_1.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.2103 - matthews_correlation: 0.0214 - val_loss: 0.2240 - val_matthews_correlation: 0.0799\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.00000 to 0.07993, saving model to weights_1.h5\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1983 - matthews_correlation: 0.1683 - val_loss: 0.2218 - val_matthews_correlation: 0.1370\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.07993 to 0.13697, saving model to weights_1.h5\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1877 - matthews_correlation: 0.2698 - val_loss: 0.1980 - val_matthews_correlation: 0.1562\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.13697 to 0.15625, saving model to weights_1.h5\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1724 - matthews_correlation: 0.2978 - val_loss: 0.1794 - val_matthews_correlation: 0.0986\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.15625\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1640 - matthews_correlation: 0.2047 - val_loss: 0.1703 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.15625\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1531 - matthews_correlation: 0.2191 - val_loss: 0.1571 - val_matthews_correlation: 0.1562\n",
      "\n",
      "Epoch 00007: val_matthews_correlation did not improve from 0.15625\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1425 - matthews_correlation: 0.1455 - val_loss: 0.1589 - val_matthews_correlation: 0.5703\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.15625 to 0.57033, saving model to weights_1.h5\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1417 - matthews_correlation: 0.3316 - val_loss: 0.1803 - val_matthews_correlation: 0.1470\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.57033\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1550 - matthews_correlation: 0.3307 - val_loss: 0.1377 - val_matthews_correlation: 0.1036\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.57033\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1420 - matthews_correlation: 0.2884 - val_loss: 0.1379 - val_matthews_correlation: 0.1562\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.57033\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1310 - matthews_correlation: 0.3688 - val_loss: 0.1414 - val_matthews_correlation: 0.5998\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.57033 to 0.59977, saving model to weights_1.h5\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1252 - matthews_correlation: 0.4734 - val_loss: 0.1246 - val_matthews_correlation: 0.5878\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.59977\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1236 - matthews_correlation: 0.4679 - val_loss: 0.1217 - val_matthews_correlation: 0.6974\n",
      "\n",
      "Epoch 00014: val_matthews_correlation improved from 0.59977 to 0.69737, saving model to weights_1.h5\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1089 - matthews_correlation: 0.5372 - val_loss: 0.1366 - val_matthews_correlation: 0.4669\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1123 - matthews_correlation: 0.5370 - val_loss: 0.1161 - val_matthews_correlation: 0.6407\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1094 - matthews_correlation: 0.5142 - val_loss: 0.1131 - val_matthews_correlation: 0.5865\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1074 - matthews_correlation: 0.6101 - val_loss: 0.1184 - val_matthews_correlation: 0.6037\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1102 - matthews_correlation: 0.5567 - val_loss: 0.1432 - val_matthews_correlation: 0.5861\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1128 - matthews_correlation: 0.5565 - val_loss: 0.1203 - val_matthews_correlation: 0.5278\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.1046 - matthews_correlation: 0.5367 - val_loss: 0.1118 - val_matthews_correlation: 0.6413\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.1003 - matthews_correlation: 0.5949 - val_loss: 0.1335 - val_matthews_correlation: 0.5805\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0998 - matthews_correlation: 0.6194 - val_loss: 0.1063 - val_matthews_correlation: 0.6631\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1055 - matthews_correlation: 0.6558 - val_loss: 0.1266 - val_matthews_correlation: 0.5187\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1127 - matthews_correlation: 0.4861 - val_loss: 0.1074 - val_matthews_correlation: 0.6550\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.69737\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0965 - matthews_correlation: 0.5903 - val_loss: 0.1029 - val_matthews_correlation: 0.7210\n",
      "\n",
      "Epoch 00026: val_matthews_correlation improved from 0.69737 to 0.72099, saving model to weights_1.h5\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0992 - matthews_correlation: 0.5540 - val_loss: 0.1313 - val_matthews_correlation: 0.5970\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.72099\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1003 - matthews_correlation: 0.6289 - val_loss: 0.1047 - val_matthews_correlation: 0.6869\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.72099\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1006 - matthews_correlation: 0.6112 - val_loss: 0.1284 - val_matthews_correlation: 0.6219\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.72099\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0998 - matthews_correlation: 0.5980 - val_loss: 0.1052 - val_matthews_correlation: 0.6474\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.72099\n",
      "Epoch 31/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0938 - matthews_correlation: 0.6301 - val_loss: 0.1038 - val_matthews_correlation: 0.7222\n",
      "\n",
      "Epoch 00031: val_matthews_correlation improved from 0.72099 to 0.72216, saving model to weights_1.h5\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0931 - matthews_correlation: 0.6480 - val_loss: 0.1292 - val_matthews_correlation: 0.6379\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.72216\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1011 - matthews_correlation: 0.6306 - val_loss: 0.1060 - val_matthews_correlation: 0.7253\n",
      "\n",
      "Epoch 00033: val_matthews_correlation improved from 0.72216 to 0.72527, saving model to weights_1.h5\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0929 - matthews_correlation: 0.6654 - val_loss: 0.1065 - val_matthews_correlation: 0.6371\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0998 - matthews_correlation: 0.6047 - val_loss: 0.1042 - val_matthews_correlation: 0.6743\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0904 - matthews_correlation: 0.5781 - val_loss: 0.1109 - val_matthews_correlation: 0.6546\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1010 - matthews_correlation: 0.6312 - val_loss: 0.1467 - val_matthews_correlation: 0.3754\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1115 - matthews_correlation: 0.5553 - val_loss: 0.1087 - val_matthews_correlation: 0.7102\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0998 - matthews_correlation: 0.5866 - val_loss: 0.1037 - val_matthews_correlation: 0.6529\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0962 - matthews_correlation: 0.5960 - val_loss: 0.0995 - val_matthews_correlation: 0.6701\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0915 - matthews_correlation: 0.6405 - val_loss: 0.1070 - val_matthews_correlation: 0.6507\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0939 - matthews_correlation: 0.6575 - val_loss: 0.1004 - val_matthews_correlation: 0.6564\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0952 - matthews_correlation: 0.5914 - val_loss: 0.1011 - val_matthews_correlation: 0.6434\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1011 - matthews_correlation: 0.5445 - val_loss: 0.1100 - val_matthews_correlation: 0.6116\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0955 - matthews_correlation: 0.6504 - val_loss: 0.1108 - val_matthews_correlation: 0.6252\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1023 - matthews_correlation: 0.6127 - val_loss: 0.0990 - val_matthews_correlation: 0.6215\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0959 - matthews_correlation: 0.6465 - val_loss: 0.1007 - val_matthews_correlation: 0.7149\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0930 - matthews_correlation: 0.6215 - val_loss: 0.1120 - val_matthews_correlation: 0.6480\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0940 - matthews_correlation: 0.6536 - val_loss: 0.1094 - val_matthews_correlation: 0.6088\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.72527\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0907 - matthews_correlation: 0.6125 - val_loss: 0.1089 - val_matthews_correlation: 0.6763\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.72527\n",
      "Beginning fold 3\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "Train on 2323 samples, validate on 581 samples\n",
      "Epoch 1/50\n",
      "2323/2323 [==============================] - 7s 3ms/step - loss: 0.2908 - matthews_correlation: 0.0000e+00 - val_loss: 0.2187 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_2.h5\n",
      "Epoch 2/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.2065 - matthews_correlation: 0.0730 - val_loss: 0.1991 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1884 - matthews_correlation: 0.0194 - val_loss: 0.1733 - val_matthews_correlation: 0.1433\n",
      "\n",
      "Epoch 00003: val_matthews_correlation improved from 0.00000 to 0.14333, saving model to weights_2.h5\n",
      "Epoch 4/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1714 - matthews_correlation: 0.1939 - val_loss: 0.1634 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00004: val_matthews_correlation did not improve from 0.14333\n",
      "Epoch 5/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1620 - matthews_correlation: 0.1421 - val_loss: 0.1449 - val_matthews_correlation: 0.1614\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.14333 to 0.16144, saving model to weights_2.h5\n",
      "Epoch 6/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1422 - matthews_correlation: 0.1673 - val_loss: 0.1526 - val_matthews_correlation: 0.2151\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.16144 to 0.21510, saving model to weights_2.h5\n",
      "Epoch 7/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1328 - matthews_correlation: 0.3679 - val_loss: 0.1382 - val_matthews_correlation: 0.3513\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.21510 to 0.35132, saving model to weights_2.h5\n",
      "Epoch 8/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1231 - matthews_correlation: 0.5096 - val_loss: 0.1358 - val_matthews_correlation: 0.3128\n",
      "\n",
      "Epoch 00008: val_matthews_correlation did not improve from 0.35132\n",
      "Epoch 9/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1207 - matthews_correlation: 0.5232 - val_loss: 0.1648 - val_matthews_correlation: 0.2456\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.35132\n",
      "Epoch 10/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1272 - matthews_correlation: 0.5130 - val_loss: 0.1389 - val_matthews_correlation: 0.4569\n",
      "\n",
      "Epoch 00010: val_matthews_correlation improved from 0.35132 to 0.45692, saving model to weights_2.h5\n",
      "Epoch 11/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1300 - matthews_correlation: 0.5083 - val_loss: 0.1325 - val_matthews_correlation: 0.3415\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.45692\n",
      "Epoch 12/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1202 - matthews_correlation: 0.5424 - val_loss: 0.1381 - val_matthews_correlation: 0.2965\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.45692\n",
      "Epoch 13/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1182 - matthews_correlation: 0.5506 - val_loss: 0.1377 - val_matthews_correlation: 0.4316\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.45692\n",
      "Epoch 14/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1128 - matthews_correlation: 0.5657 - val_loss: 0.1350 - val_matthews_correlation: 0.3606\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.45692\n",
      "Epoch 15/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1146 - matthews_correlation: 0.5142 - val_loss: 0.1612 - val_matthews_correlation: 0.5507\n",
      "\n",
      "Epoch 00015: val_matthews_correlation improved from 0.45692 to 0.55070, saving model to weights_2.h5\n",
      "Epoch 16/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1226 - matthews_correlation: 0.5615 - val_loss: 0.1724 - val_matthews_correlation: 0.1635\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 17/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1162 - matthews_correlation: 0.5080 - val_loss: 0.1442 - val_matthews_correlation: 0.4159\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 18/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1155 - matthews_correlation: 0.5414 - val_loss: 0.1310 - val_matthews_correlation: 0.4093\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 19/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1080 - matthews_correlation: 0.5843 - val_loss: 0.1392 - val_matthews_correlation: 0.4803\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 20/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1125 - matthews_correlation: 0.5893 - val_loss: 0.1318 - val_matthews_correlation: 0.3731\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 21/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1133 - matthews_correlation: 0.6195 - val_loss: 0.1283 - val_matthews_correlation: 0.3952\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 22/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1046 - matthews_correlation: 0.5883 - val_loss: 0.1309 - val_matthews_correlation: 0.4194\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 23/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1063 - matthews_correlation: 0.6099 - val_loss: 0.1315 - val_matthews_correlation: 0.4344\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 24/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1131 - matthews_correlation: 0.6113 - val_loss: 0.1262 - val_matthews_correlation: 0.3893\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 25/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1052 - matthews_correlation: 0.6012 - val_loss: 0.1271 - val_matthews_correlation: 0.5031\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 26/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0986 - matthews_correlation: 0.6308 - val_loss: 0.1279 - val_matthews_correlation: 0.4667\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 27/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0970 - matthews_correlation: 0.6309 - val_loss: 0.1284 - val_matthews_correlation: 0.3952\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 28/50\n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.0974 - matthews_correlation: 0.6386 - val_loss: 0.1339 - val_matthews_correlation: 0.3811\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 29/50\n",
      "2323/2323 [==============================] - 6s 3ms/step - loss: 0.0943 - matthews_correlation: 0.6521 - val_loss: 0.1240 - val_matthews_correlation: 0.5116\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.55070\n",
      "Epoch 30/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0980 - matthews_correlation: 0.6478 - val_loss: 0.1284 - val_matthews_correlation: 0.5686\n",
      "\n",
      "Epoch 00030: val_matthews_correlation improved from 0.55070 to 0.56864, saving model to weights_2.h5\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0936 - matthews_correlation: 0.6505 - val_loss: 0.1261 - val_matthews_correlation: 0.4131\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 32/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0912 - matthews_correlation: 0.6638 - val_loss: 0.1277 - val_matthews_correlation: 0.4667\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 33/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0989 - matthews_correlation: 0.5936 - val_loss: 0.1247 - val_matthews_correlation: 0.4363\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 34/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0991 - matthews_correlation: 0.6085 - val_loss: 0.1402 - val_matthews_correlation: 0.3832\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 35/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0952 - matthews_correlation: 0.6433 - val_loss: 0.1561 - val_matthews_correlation: 0.3882\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 36/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1018 - matthews_correlation: 0.6037 - val_loss: 0.1266 - val_matthews_correlation: 0.5466\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 37/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0930 - matthews_correlation: 0.6147 - val_loss: 0.1247 - val_matthews_correlation: 0.4194\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 38/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0877 - matthews_correlation: 0.6312 - val_loss: 0.1313 - val_matthews_correlation: 0.4885\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 39/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0914 - matthews_correlation: 0.6614 - val_loss: 0.1283 - val_matthews_correlation: 0.4742\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.56864\n",
      "Epoch 40/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0887 - matthews_correlation: 0.6439 - val_loss: 0.1301 - val_matthews_correlation: 0.6524\n",
      "\n",
      "Epoch 00040: val_matthews_correlation improved from 0.56864 to 0.65238, saving model to weights_2.h5\n",
      "Epoch 41/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1027 - matthews_correlation: 0.6158 - val_loss: 0.1527 - val_matthews_correlation: 0.2811\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 42/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0992 - matthews_correlation: 0.6091 - val_loss: 0.1325 - val_matthews_correlation: 0.4885\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 43/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0873 - matthews_correlation: 0.6676 - val_loss: 0.1225 - val_matthews_correlation: 0.4967\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 44/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0900 - matthews_correlation: 0.6743 - val_loss: 0.1357 - val_matthews_correlation: 0.4389\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 45/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0895 - matthews_correlation: 0.6585 - val_loss: 0.1223 - val_matthews_correlation: 0.4294\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 46/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0940 - matthews_correlation: 0.6322 - val_loss: 0.1507 - val_matthews_correlation: 0.3452\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 47/50\n",
      "2323/2323 [==============================] - 6s 2ms/step - loss: 0.0945 - matthews_correlation: 0.6452 - val_loss: 0.1247 - val_matthews_correlation: 0.4442\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 48/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0886 - matthews_correlation: 0.6780 - val_loss: 0.1446 - val_matthews_correlation: 0.4052\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 49/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.0980 - matthews_correlation: 0.6079 - val_loss: 0.1363 - val_matthews_correlation: 0.5985\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.65238\n",
      "Epoch 50/50\n",
      "2323/2323 [==============================] - 5s 2ms/step - loss: 0.1056 - matthews_correlation: 0.5688 - val_loss: 0.1371 - val_matthews_correlation: 0.4146\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.65238\n",
      "Beginning fold 4\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 7s 3ms/step - loss: 0.3204 - matthews_correlation: 0.0297 - val_loss: 0.2233 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_3.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 6s 3ms/step - loss: 0.2080 - matthews_correlation: -6.2113e-04 - val_loss: 0.1783 - val_matthews_correlation: 0.0432\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.00000 to 0.04325, saving model to weights_3.h5\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1764 - matthews_correlation: 0.1253 - val_loss: 0.1667 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.04325\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1617 - matthews_correlation: 0.0523 - val_loss: 0.1506 - val_matthews_correlation: 0.0737\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.04325 to 0.07370, saving model to weights_3.h5\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1485 - matthews_correlation: 0.1290 - val_loss: 0.1451 - val_matthews_correlation: 0.2665\n",
      "\n",
      "Epoch 00005: val_matthews_correlation improved from 0.07370 to 0.26648, saving model to weights_3.h5\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1397 - matthews_correlation: 0.3105 - val_loss: 0.1466 - val_matthews_correlation: 0.3733\n",
      "\n",
      "Epoch 00006: val_matthews_correlation improved from 0.26648 to 0.37335, saving model to weights_3.h5\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1373 - matthews_correlation: 0.4171 - val_loss: 0.1313 - val_matthews_correlation: 0.3904\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.37335 to 0.39041, saving model to weights_3.h5\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1217 - matthews_correlation: 0.4710 - val_loss: 0.1256 - val_matthews_correlation: 0.5473\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.39041 to 0.54734, saving model to weights_3.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1219 - matthews_correlation: 0.5395 - val_loss: 0.1256 - val_matthews_correlation: 0.5551\n",
      "\n",
      "Epoch 00009: val_matthews_correlation improved from 0.54734 to 0.55511, saving model to weights_3.h5\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1137 - matthews_correlation: 0.5590 - val_loss: 0.1351 - val_matthews_correlation: 0.3831\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.55511\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1309 - matthews_correlation: 0.5032 - val_loss: 0.1318 - val_matthews_correlation: 0.3388\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.55511\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1213 - matthews_correlation: 0.5014 - val_loss: 0.1452 - val_matthews_correlation: 0.5874\n",
      "\n",
      "Epoch 00012: val_matthews_correlation improved from 0.55511 to 0.58745, saving model to weights_3.h5\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1340 - matthews_correlation: 0.5228 - val_loss: 0.1181 - val_matthews_correlation: 0.4931\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.58745\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1231 - matthews_correlation: 0.4476 - val_loss: 0.1135 - val_matthews_correlation: 0.5010\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.58745\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1173 - matthews_correlation: 0.5516 - val_loss: 0.1185 - val_matthews_correlation: 0.5020\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.58745\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1131 - matthews_correlation: 0.6159 - val_loss: 0.1246 - val_matthews_correlation: 0.5997\n",
      "\n",
      "Epoch 00016: val_matthews_correlation improved from 0.58745 to 0.59966, saving model to weights_3.h5\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1072 - matthews_correlation: 0.5779 - val_loss: 0.1299 - val_matthews_correlation: 0.4643\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.59966\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1114 - matthews_correlation: 0.6440 - val_loss: 0.1087 - val_matthews_correlation: 0.5364\n",
      "\n",
      "Epoch 00018: val_matthews_correlation did not improve from 0.59966\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1071 - matthews_correlation: 0.5583 - val_loss: 0.1057 - val_matthews_correlation: 0.5914\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.59966\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1059 - matthews_correlation: 0.6045 - val_loss: 0.1156 - val_matthews_correlation: 0.5145\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.59966\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1034 - matthews_correlation: 0.6214 - val_loss: 0.1057 - val_matthews_correlation: 0.6143\n",
      "\n",
      "Epoch 00021: val_matthews_correlation improved from 0.59966 to 0.61429, saving model to weights_3.h5\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1004 - matthews_correlation: 0.6164 - val_loss: 0.1061 - val_matthews_correlation: 0.6021\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.61429\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1037 - matthews_correlation: 0.6188 - val_loss: 0.1027 - val_matthews_correlation: 0.5608\n",
      "\n",
      "Epoch 00023: val_matthews_correlation did not improve from 0.61429\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0991 - matthews_correlation: 0.5969 - val_loss: 0.1030 - val_matthews_correlation: 0.5892\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.61429\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0981 - matthews_correlation: 0.6264 - val_loss: 0.1002 - val_matthews_correlation: 0.5792\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.61429\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1052 - matthews_correlation: 0.5724 - val_loss: 0.1036 - val_matthews_correlation: 0.6393\n",
      "\n",
      "Epoch 00026: val_matthews_correlation improved from 0.61429 to 0.63928, saving model to weights_3.h5\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0966 - matthews_correlation: 0.6356 - val_loss: 0.1036 - val_matthews_correlation: 0.5882\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.63928\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0968 - matthews_correlation: 0.6760 - val_loss: 0.1011 - val_matthews_correlation: 0.6483\n",
      "\n",
      "Epoch 00028: val_matthews_correlation improved from 0.63928 to 0.64830, saving model to weights_3.h5\n",
      "Epoch 29/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0944 - matthews_correlation: 0.6232 - val_loss: 0.1005 - val_matthews_correlation: 0.6277\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.64830\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1000 - matthews_correlation: 0.6150 - val_loss: 0.1277 - val_matthews_correlation: 0.5095\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.64830\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0996 - matthews_correlation: 0.6164 - val_loss: 0.0977 - val_matthews_correlation: 0.6788\n",
      "\n",
      "Epoch 00031: val_matthews_correlation improved from 0.64830 to 0.67883, saving model to weights_3.h5\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0954 - matthews_correlation: 0.6167 - val_loss: 0.1005 - val_matthews_correlation: 0.6172\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.67883\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0949 - matthews_correlation: 0.6582 - val_loss: 0.1091 - val_matthews_correlation: 0.5737\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.67883\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0992 - matthews_correlation: 0.6104 - val_loss: 0.0958 - val_matthews_correlation: 0.6864\n",
      "\n",
      "Epoch 00034: val_matthews_correlation improved from 0.67883 to 0.68637, saving model to weights_3.h5\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 6s 3ms/step - loss: 0.0915 - matthews_correlation: 0.6820 - val_loss: 0.1072 - val_matthews_correlation: 0.6146\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.68637\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0936 - matthews_correlation: 0.6652 - val_loss: 0.1179 - val_matthews_correlation: 0.6932\n",
      "\n",
      "Epoch 00036: val_matthews_correlation improved from 0.68637 to 0.69318, saving model to weights_3.h5\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1020 - matthews_correlation: 0.6290 - val_loss: 0.1033 - val_matthews_correlation: 0.6142\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1022 - matthews_correlation: 0.6401 - val_loss: 0.1107 - val_matthews_correlation: 0.5648\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1034 - matthews_correlation: 0.6039 - val_loss: 0.1148 - val_matthews_correlation: 0.6445\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0925 - matthews_correlation: 0.6496 - val_loss: 0.0979 - val_matthews_correlation: 0.6613\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0917 - matthews_correlation: 0.5977 - val_loss: 0.1191 - val_matthews_correlation: 0.6545\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1013 - matthews_correlation: 0.6659 - val_loss: 0.1047 - val_matthews_correlation: 0.5706\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1047 - matthews_correlation: 0.5766 - val_loss: 0.1038 - val_matthews_correlation: 0.6863\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0950 - matthews_correlation: 0.6407 - val_loss: 0.1015 - val_matthews_correlation: 0.5962\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0945 - matthews_correlation: 0.6190 - val_loss: 0.0996 - val_matthews_correlation: 0.6322\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0915 - matthews_correlation: 0.6241 - val_loss: 0.0978 - val_matthews_correlation: 0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0917 - matthews_correlation: 0.6271 - val_loss: 0.0975 - val_matthews_correlation: 0.6825\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0927 - matthews_correlation: 0.6502 - val_loss: 0.0929 - val_matthews_correlation: 0.6467\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0903 - matthews_correlation: 0.6555 - val_loss: 0.0980 - val_matthews_correlation: 0.6695\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.69318\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0948 - matthews_correlation: 0.6428 - val_loss: 0.1011 - val_matthews_correlation: 0.6451\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.69318\n",
      "Beginning fold 5\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 160, 160)\n",
      "(?, 160, 57)\n",
      "(?, 160, 57)\n",
      "(?, 57)\n",
      "Train on 2324 samples, validate on 580 samples\n",
      "Epoch 1/50\n",
      "2324/2324 [==============================] - 7s 3ms/step - loss: 0.3314 - matthews_correlation: 0.0087 - val_loss: 0.2271 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_4.h5\n",
      "Epoch 2/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.2147 - matthews_correlation: 0.0000e+00 - val_loss: 0.1861 - val_matthews_correlation: 0.2506\n",
      "\n",
      "Epoch 00002: val_matthews_correlation improved from 0.00000 to 0.25056, saving model to weights_4.h5\n",
      "Epoch 3/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1865 - matthews_correlation: 0.1796 - val_loss: 0.1703 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.25056\n",
      "Epoch 4/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1657 - matthews_correlation: 0.0566 - val_loss: 0.1490 - val_matthews_correlation: 0.4010\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.25056 to 0.40098, saving model to weights_4.h5\n",
      "Epoch 5/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1569 - matthews_correlation: 0.1333 - val_loss: 0.1401 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.40098\n",
      "Epoch 6/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1331 - matthews_correlation: 0.0461 - val_loss: 0.1258 - val_matthews_correlation: 0.2854\n",
      "\n",
      "Epoch 00006: val_matthews_correlation did not improve from 0.40098\n",
      "Epoch 7/50\n",
      "2324/2324 [==============================] - 6s 3ms/step - loss: 0.1306 - matthews_correlation: 0.4041 - val_loss: 0.1336 - val_matthews_correlation: 0.4169\n",
      "\n",
      "Epoch 00007: val_matthews_correlation improved from 0.40098 to 0.41687, saving model to weights_4.h5\n",
      "Epoch 8/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1226 - matthews_correlation: 0.5788 - val_loss: 0.1221 - val_matthews_correlation: 0.4915\n",
      "\n",
      "Epoch 00008: val_matthews_correlation improved from 0.41687 to 0.49153, saving model to weights_4.h5\n",
      "Epoch 9/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1183 - matthews_correlation: 0.4616 - val_loss: 0.1213 - val_matthews_correlation: 0.3845\n",
      "\n",
      "Epoch 00009: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 10/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1108 - matthews_correlation: 0.5628 - val_loss: 0.1151 - val_matthews_correlation: 0.4449\n",
      "\n",
      "Epoch 00010: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 11/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1124 - matthews_correlation: 0.5188 - val_loss: 0.1254 - val_matthews_correlation: 0.4240\n",
      "\n",
      "Epoch 00011: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 12/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1082 - matthews_correlation: 0.5531 - val_loss: 0.1128 - val_matthews_correlation: 0.4647\n",
      "\n",
      "Epoch 00012: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 13/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1097 - matthews_correlation: 0.5410 - val_loss: 0.1270 - val_matthews_correlation: 0.3735\n",
      "\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 14/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1096 - matthews_correlation: 0.4960 - val_loss: 0.1145 - val_matthews_correlation: 0.4756\n",
      "\n",
      "Epoch 00014: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 15/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1135 - matthews_correlation: 0.5485 - val_loss: 0.1213 - val_matthews_correlation: 0.4701\n",
      "\n",
      "Epoch 00015: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 16/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1121 - matthews_correlation: 0.5226 - val_loss: 0.1092 - val_matthews_correlation: 0.4413\n",
      "\n",
      "Epoch 00016: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 17/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1015 - matthews_correlation: 0.5523 - val_loss: 0.1160 - val_matthews_correlation: 0.4625\n",
      "\n",
      "Epoch 00017: val_matthews_correlation did not improve from 0.49153\n",
      "Epoch 18/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.1027 - matthews_correlation: 0.5603 - val_loss: 0.1263 - val_matthews_correlation: 0.5024\n",
      "\n",
      "Epoch 00018: val_matthews_correlation improved from 0.49153 to 0.50239, saving model to weights_4.h5\n",
      "Epoch 19/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1089 - matthews_correlation: 0.6092 - val_loss: 0.1222 - val_matthews_correlation: 0.4129\n",
      "\n",
      "Epoch 00019: val_matthews_correlation did not improve from 0.50239\n",
      "Epoch 20/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1033 - matthews_correlation: 0.5736 - val_loss: 0.1310 - val_matthews_correlation: 0.4945\n",
      "\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.50239\n",
      "Epoch 21/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0992 - matthews_correlation: 0.6067 - val_loss: 0.1191 - val_matthews_correlation: 0.5008\n",
      "\n",
      "Epoch 00021: val_matthews_correlation did not improve from 0.50239\n",
      "Epoch 22/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1038 - matthews_correlation: 0.5875 - val_loss: 0.1124 - val_matthews_correlation: 0.4860\n",
      "\n",
      "Epoch 00022: val_matthews_correlation did not improve from 0.50239\n",
      "Epoch 23/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1030 - matthews_correlation: 0.5625 - val_loss: 0.1234 - val_matthews_correlation: 0.5864\n",
      "\n",
      "Epoch 00023: val_matthews_correlation improved from 0.50239 to 0.58639, saving model to weights_4.h5\n",
      "Epoch 24/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0999 - matthews_correlation: 0.6087 - val_loss: 0.1099 - val_matthews_correlation: 0.4405\n",
      "\n",
      "Epoch 00024: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 25/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1110 - matthews_correlation: 0.5574 - val_loss: 0.1192 - val_matthews_correlation: 0.4402\n",
      "\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 26/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1142 - matthews_correlation: 0.5234 - val_loss: 0.1312 - val_matthews_correlation: 0.4485\n",
      "\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 27/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0980 - matthews_correlation: 0.5462 - val_loss: 0.1154 - val_matthews_correlation: 0.4613\n",
      "\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 28/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0961 - matthews_correlation: 0.5892 - val_loss: 0.1117 - val_matthews_correlation: 0.5266\n",
      "\n",
      "Epoch 00028: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0924 - matthews_correlation: 0.6319 - val_loss: 0.1122 - val_matthews_correlation: 0.4553\n",
      "\n",
      "Epoch 00029: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 30/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0901 - matthews_correlation: 0.6236 - val_loss: 0.1072 - val_matthews_correlation: 0.5110\n",
      "\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 31/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0895 - matthews_correlation: 0.6593 - val_loss: 0.1159 - val_matthews_correlation: 0.4612\n",
      "\n",
      "Epoch 00031: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 32/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0897 - matthews_correlation: 0.6554 - val_loss: 0.1162 - val_matthews_correlation: 0.5069\n",
      "\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 33/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0910 - matthews_correlation: 0.6185 - val_loss: 0.1097 - val_matthews_correlation: 0.5119\n",
      "\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 34/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0893 - matthews_correlation: 0.6250 - val_loss: 0.1105 - val_matthews_correlation: 0.4826\n",
      "\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 35/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0976 - matthews_correlation: 0.6222 - val_loss: 0.1302 - val_matthews_correlation: 0.3527\n",
      "\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 36/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0984 - matthews_correlation: 0.6157 - val_loss: 0.1095 - val_matthews_correlation: 0.4615\n",
      "\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 37/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0895 - matthews_correlation: 0.6002 - val_loss: 0.1202 - val_matthews_correlation: 0.5069\n",
      "\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 38/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0920 - matthews_correlation: 0.6419 - val_loss: 0.1114 - val_matthews_correlation: 0.4517\n",
      "\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 39/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0887 - matthews_correlation: 0.6169 - val_loss: 0.1117 - val_matthews_correlation: 0.4476\n",
      "\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 40/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0878 - matthews_correlation: 0.6160 - val_loss: 0.1151 - val_matthews_correlation: 0.4862\n",
      "\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 41/50\n",
      "2324/2324 [==============================] - 6s 3ms/step - loss: 0.0942 - matthews_correlation: 0.6336 - val_loss: 0.1147 - val_matthews_correlation: 0.4712\n",
      "\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 42/50\n",
      "2324/2324 [==============================] - 6s 2ms/step - loss: 0.0941 - matthews_correlation: 0.6339 - val_loss: 0.1132 - val_matthews_correlation: 0.4374\n",
      "\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 43/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0913 - matthews_correlation: 0.6422 - val_loss: 0.1124 - val_matthews_correlation: 0.4787\n",
      "\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 44/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0910 - matthews_correlation: 0.6062 - val_loss: 0.1248 - val_matthews_correlation: 0.4287\n",
      "\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 45/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0906 - matthews_correlation: 0.6078 - val_loss: 0.1078 - val_matthews_correlation: 0.4684\n",
      "\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 46/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0852 - matthews_correlation: 0.6564 - val_loss: 0.1117 - val_matthews_correlation: 0.4613\n",
      "\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 47/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0877 - matthews_correlation: 0.5974 - val_loss: 0.1158 - val_matthews_correlation: 0.4615\n",
      "\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 48/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0924 - matthews_correlation: 0.6142 - val_loss: 0.1156 - val_matthews_correlation: 0.4884\n",
      "\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 49/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.0966 - matthews_correlation: 0.6012 - val_loss: 0.1100 - val_matthews_correlation: 0.4738\n",
      "\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.58639\n",
      "Epoch 50/50\n",
      "2324/2324 [==============================] - 5s 2ms/step - loss: 0.1010 - matthews_correlation: 0.5300 - val_loss: 0.1470 - val_matthews_correlation: 0.3954\n",
      "\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.58639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2904,), (2904,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "# First, create a set of indexes of the 5 folds\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    model.fit(train_X, train_y, batch_size=150, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "preds_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tmp/y_val_tmp_8.npy', y_val)\n",
    "np.save('./tmp/preds_val_tmp_8.npy', preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_uuid": "c3340ee96becb5ca8f075d9c44b7df383ddba5ee"
   },
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    \n",
    "    #y_pred = K.cast(y_pred, np.float)\n",
    "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = np.sum(y_pos * y_pred_pos)\n",
    "    tn = np.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = np.sum(y_neg * y_pred_pos)\n",
    "    fn = np.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_uuid": "d28151fd0be9fd9762f3f55e307d82f89bfbd291"
   },
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)]):\n",
    "        #score = matthews_correlation(y_true, (y_proba > threshold).astype(int))\n",
    "        #score = K.eval(matthews_correlation(y_true, (y_proba > threshold).astype(int)))\n",
    "        #score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        score = matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "6fee7f722ed08bc1453a822a4371ed2d48e08abc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 7696.82it/s]\n"
     ]
    }
   ],
   "source": [
    "best_threshold = threshold_search(y_val, preds_val)['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6883714509140777"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_correlation(y_val, preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 14110.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_search(y_val[:582], preds_val[:582])['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 12112.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_search(y_val[582:582+581], preds_val[582:582+581])['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 15628.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_search(y_val[582+581:582+581*2], preds_val[582+581:582+581*2])['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 15360.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_search(y_val[582+581*2:582+581*2+580], preds_val[582+581*2:582+581*2+580])['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 15026.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_search(y_val[582+581*2+580:], preds_val[582+581*2+580:])['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "ae9bd3fa9d8c0781c0708846bb7f2a9f9e6cbd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 ms, sys: 8.27 ms, total: 42.5 ms\n",
      "Wall time: 71.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now load the test data\n",
    "# This first part is the meta data, not the main data, the measurements\n",
    "meta_test = pd.read_csv('../input/metadata_test.csv')\n",
    "df_test = meta_test.set_index(['id_measurement', 'phase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "3eb186d032f79c99ffba05dd1a7fabb77e13cec5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>signal_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>2904</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>2904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>2904</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>2905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>2905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id_measurement  phase\n",
       "signal_id                       \n",
       "8712                 2904      0\n",
       "8713                 2904      1\n",
       "8714                 2904      2\n",
       "8715                 2905      0\n",
       "8716                 2905      1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test = meta_test.set_index(['signal_id'])\n",
    "meta_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_uuid": "7460e718a605803f1d9e4fbec61750a0deb02a47"
   },
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data_test(start, end):\n",
    "    # load a piece of data from file\n",
    "    praq_test = pq.read_pandas('../input/test.parquet', columns=[str(i) for i in range(start+8712, end+8712)]).to_pandas()\n",
    "    X = []\n",
    "\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_test.index.levels[0].unique()[int(start/3):int(end/3)]):\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id = df_test.loc[id_measurement].loc[phase][0]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            #if phase == 0:\n",
    "                #ts_1 = min_max_transf(praq_test[str(signal_id)], min_data=min_num, max_data=max_num)\n",
    "                #ts_2 = min_max_transf(praq_test[str(signal_id+1)], min_data=min_num, max_data=max_num)\n",
    "                #ts_3 = min_max_transf(praq_test[str(signal_id+2)], min_data=min_num, max_data=max_num)\n",
    "                \n",
    "                #ts_wave_1 = denoise_signal(ts_1, wavelet='haar', level=1)\n",
    "                #ts_wave_2 = denoise_signal(ts_2, wavelet='haar', level=1)\n",
    "                #ts_wave_3 = denoise_signal(ts_3, wavelet='haar', level=1)\n",
    "                \n",
    "                #ts_rm_1, _ = remove_corona(ts_wave_1)\n",
    "                #ts_rm_2, _ = remove_corona(ts_wave_2)\n",
    "                #ts_rm_3, _ = remove_corona(ts_wave_3)\n",
    "                \n",
    "                #ts_sum = ts_rm_1 + ts_rm_2 + ts_rm_3\n",
    "                #X_signal.append(transform_ts_sum(ts_sum))\n",
    "            # extract and transform data into sets of features\n",
    "            X_signal.append(transform_ts(praq_test[str(signal_id)]))\n",
    "        # concatenate all the 3 phases in one matrix\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subtest(arg_tuple):\n",
    "    start, end, idx = arg_tuple\n",
    "    X = prep_data_test(start, end)\n",
    "    return idx, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "num_cores = 16 \n",
    "#def load_all():\n",
    "total_size = len(meta_test)\n",
    "chunk_size = np.ceil(total_size/num_cores)\n",
    "#train_size = len(df_train)\n",
    "\n",
    "for i in range(16):\n",
    "    if i != 15:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(start_idx + chunk_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "    else:\n",
    "        start_idx = int(i * chunk_size)\n",
    "        end_idx = int(total_size)\n",
    "        #chunk = (start_idx+train_size, end_idx+train_size, i)\n",
    "        chunk = (start_idx, end_idx, i)\n",
    "        all_chunks.append(chunk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 424/424 [02:49<00:00,  2.55it/s]\n",
      "100%|██████████| 424/424 [02:49<00:00,  2.47it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.52it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.52it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.49it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.66it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.80it/s]\n",
      "100%|██████████| 424/424 [02:50<00:00,  2.84it/s]\n",
      "100%|██████████| 419/419 [02:45<00:00,  2.58it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  2.64it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  2.70it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  2.80it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  2.84it/s]\n",
      "100%|██████████| 424/424 [02:47<00:00,  2.93it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  3.39it/s]\n",
      "100%|██████████| 424/424 [02:48<00:00,  3.39it/s]\n"
     ]
    }
   ],
   "source": [
    "pool = Pool()\n",
    "results_1 = pool.map(process_subtest, all_chunks[0:8])    \n",
    "results_1 = sorted(results_1, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = pool.map(process_subtest, all_chunks[8:16])    \n",
    "results_2 = sorted(results_2, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results_1 + results_2\n",
    "X_test = np.concatenate([item[1] for item in results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_test_7.npy\",X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_uuid": "cfd265d3e07c4cc1679d2c4d55fe7de631c813e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20337\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "print(len(submission))\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y))\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[np.isnan(X_test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"X_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2904, 160, 57)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6779, 160, 57)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.14322358,  0.00713458,  0.15035816, ...,  0.00510432,\n",
       "          0.02079059,  0.03647687],\n",
       "        [ 0.14152163,  0.00867319,  0.15019482, ...,  0.00490825,\n",
       "          0.02059452,  0.02843766],\n",
       "        [ 0.1306479 ,  0.00819584,  0.13884374, ...,  0.00410981,\n",
       "          0.01979609,  0.01979609],\n",
       "        ...,\n",
       "        [ 0.1508832 ,  0.00819278,  0.15907598, ...,  0.00390902,\n",
       "          0.0195953 ,  0.02743843],\n",
       "        [ 0.148993  ,  0.00857161,  0.15756461, ...,  0.00399999,\n",
       "          0.01968627,  0.03537254],\n",
       "        [ 0.145873  ,  0.00776367,  0.15363666, ...,  0.00493018,\n",
       "          0.02061646,  0.02845959]],\n",
       "\n",
       "       [[-0.1186494 ,  0.00703992, -0.11160948, ...,  0.00143373,\n",
       "          0.01712   ,  0.06417882],\n",
       "        [-0.11164077,  0.0072766 , -0.10436417, ...,  0.0018902 ,\n",
       "          0.01757647,  0.10385098],\n",
       "        [-0.10408157,  0.0066795 , -0.09740207, ...,  0.00229335,\n",
       "          0.01797962,  0.0336659 ],\n",
       "        ...,\n",
       "        [-0.13483763,  0.00655647, -0.12828116, ...,  0.00063215,\n",
       "          0.01631843,  0.02416156],\n",
       "        [-0.1304392 ,  0.00666546, -0.12377375, ...,  0.00436236,\n",
       "          0.0122055 ,  0.05142118],\n",
       "        [-0.12415842,  0.00669552, -0.1174629 , ...,  0.00207528,\n",
       "          0.01776156,  0.03344783]],\n",
       "\n",
       "       [[-0.12425254,  0.01233219, -0.11192034, ...,  0.01091767,\n",
       "          0.02660395,  0.02660395],\n",
       "        [-0.12889412,  0.0130584 , -0.11583573, ...,  0.0078902 ,\n",
       "          0.02357647,  0.03926275],\n",
       "        [-0.13107136,  0.01193916, -0.11913219, ...,  0.00748392,\n",
       "          0.0231702 ,  0.03101333],\n",
       "        ...,\n",
       "        [-0.10479528,  0.01299831, -0.09179698, ...,  0.00599843,\n",
       "          0.0295279 ,  0.03737104],\n",
       "        [-0.11835449,  0.01210352, -0.10625097, ...,  0.00649258,\n",
       "          0.02217886,  0.030022  ],\n",
       "        [-0.11685333,  0.0125827 , -0.10427064, ...,  0.00619141,\n",
       "          0.02187768,  0.02972082]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.15933332,  0.01054712, -0.14878619, ...,  0.0057255 ,\n",
       "          0.01364706,  0.02141177],\n",
       "        [-0.15976313,  0.01166928, -0.14809385, ...,  0.00624941,\n",
       "          0.02193569,  0.16311215],\n",
       "        [-0.16335686,  0.00998889, -0.15336797, ...,  0.00111059,\n",
       "          0.01679686,  0.02464   ],\n",
       "        ...,\n",
       "        [-0.1564047 ,  0.0125436 , -0.1438611 , ...,  0.00264942,\n",
       "          0.01833569,  0.11245333],\n",
       "        [-0.16053647,  0.01274118, -0.14779529, ...,  0.00468236,\n",
       "          0.02036863,  0.04389804],\n",
       "        [-0.16187136,  0.01108463, -0.15078673, ...,  0.00679687,\n",
       "          0.01464   ,  0.02248314]],\n",
       "\n",
       "       [[ 0.10893653,  0.00687771,  0.11581424, ...,  0.00427921,\n",
       "          0.01996548,  0.05918117],\n",
       "        [ 0.11337261,  0.00622221,  0.11959482, ...,  0.00283293,\n",
       "          0.01851921,  0.02636234],\n",
       "        [ 0.110502  ,  0.00642984,  0.11693184, ...,  0.00600314,\n",
       "          0.02168941,  0.02953255],\n",
       "        ...,\n",
       "        [ 0.10733652,  0.00619274,  0.11352926, ...,  0.00403608,\n",
       "          0.01972236,  0.03540863],\n",
       "        [ 0.1099875 ,  0.00627967,  0.11626717, ...,  0.00515765,\n",
       "          0.02084392,  0.02868706],\n",
       "        [ 0.10780868,  0.00587597,  0.11368465, ...,  0.00445962,\n",
       "          0.02014589,  0.02798903]],\n",
       "\n",
       "       [[-0.10847842,  0.00558704, -0.10289138, ...,  0.00606275,\n",
       "          0.01390588,  0.02959216],\n",
       "        [-0.10711216,  0.00582832, -0.10128384, ...,  0.00544314,\n",
       "          0.01328628,  0.02112941],\n",
       "        [-0.0936345 ,  0.00839108, -0.08524343, ...,  0.0054604 ,\n",
       "          0.02114667,  0.02898981],\n",
       "        ...,\n",
       "        [-0.10980704,  0.0062253 , -0.10358175, ...,  0.00432   ,\n",
       "          0.01216313,  0.02000627],\n",
       "        [-0.1089898 ,  0.00601843, -0.10297137, ...,  0.00548078,\n",
       "          0.01332392,  0.02116705],\n",
       "        [-0.10922039,  0.0057296 , -0.10349079, ...,  0.00853333,\n",
       "          0.01637647,  0.0242196 ]]], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.12737725,  0.00671154,  0.13408879, ...,  0.00604392,\n",
       "          0.01388706,  0.06878902],\n",
       "        [ 0.13553725,  0.00676144,  0.14229869, ...,  0.00260078,\n",
       "          0.01828706,  0.04965961],\n",
       "        [ 0.14090824,  0.00665593,  0.14756416, ...,  0.00105569,\n",
       "          0.01674196,  0.05595765],\n",
       "        ...,\n",
       "        [ 0.1080502 ,  0.00712202,  0.11517221, ...,  0.00661333,\n",
       "          0.01445647,  0.03798588],\n",
       "        [ 0.11318431,  0.00706231,  0.12024663, ...,  0.00566118,\n",
       "          0.01350431,  0.02919059],\n",
       "        [ 0.11981333,  0.00701456,  0.12682789, ...,  0.00527686,\n",
       "          0.01312   ,  0.02096314]],\n",
       "\n",
       "       [[ 0.11695686,  0.01065462,  0.12761148, ...,  0.00762824,\n",
       "          0.02331451,  0.03900078],\n",
       "        [ 0.12716235,  0.01088737,  0.13804972, ...,  0.00403137,\n",
       "          0.01971765,  0.03540392],\n",
       "        [ 0.13502902,  0.01066675,  0.14569577, ...,  0.00606275,\n",
       "          0.02174902,  0.03743529],\n",
       "        ...,\n",
       "        [ 0.10294902,  0.01080448,  0.1137535 , ...,  0.00801255,\n",
       "          0.02369882,  0.03154196],\n",
       "        [ 0.10578667,  0.010639  ,  0.11642567, ...,  0.00756706,\n",
       "          0.02325333,  0.03893961],\n",
       "        [ 0.10824627,  0.01089983,  0.1191461 , ...,  0.00909647,\n",
       "          0.02478275,  0.04046902]],\n",
       "\n",
       "       [[-0.10054745,  0.01469543, -0.08585202, ...,  0.00876706,\n",
       "          0.02445333,  0.04013961],\n",
       "        [-0.09914039,  0.01658816, -0.08255224, ...,  0.00834196,\n",
       "          0.02402824,  0.03971451],\n",
       "        [-0.09654431,  0.01495676, -0.08158755, ...,  0.00676863,\n",
       "          0.03029804,  0.03814118],\n",
       "        ...,\n",
       "        [-0.11128   ,  0.0189326 , -0.0923474 , ...,  0.01144157,\n",
       "          0.03497098,  0.05065725],\n",
       "        [-0.10734588,  0.01685655, -0.09048933, ...,  0.01148549,\n",
       "          0.02717176,  0.04285804],\n",
       "        [-0.10635294,  0.01630479, -0.09004815, ...,  0.00920157,\n",
       "          0.02488784,  0.04057412]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.1042698 ,  0.00870403,  0.11297383, ...,  0.00450353,\n",
       "          0.02803294,  0.03587608],\n",
       "        [ 0.09425098,  0.00876174,  0.10301272, ...,  0.00758745,\n",
       "          0.02327373,  0.03896   ],\n",
       "        [ 0.08910588,  0.0085814 ,  0.09768729, ...,  0.00184941,\n",
       "          0.02537882,  0.04890824],\n",
       "        ...,\n",
       "        [ 0.1340298 ,  0.00886091,  0.14289071, ...,  0.00915922,\n",
       "          0.02484549,  0.04053176],\n",
       "        [ 0.12064627,  0.00863177,  0.12927805, ...,  0.00441882,\n",
       "          0.02794824,  0.03579137],\n",
       "        [ 0.11256471,  0.00839884,  0.12096355, ...,  0.00921098,\n",
       "          0.02489725,  0.03274039]],\n",
       "\n",
       "       [[ 0.15049412,  0.00536983,  0.15586395, ...,  0.00297882,\n",
       "          0.01082196,  0.02650824],\n",
       "        [ 0.14665255,  0.00489172,  0.15154427, ..., -0.00085647,\n",
       "          0.0148298 ,  0.03835922],\n",
       "        [ 0.14580078,  0.00459236,  0.15039314, ...,  0.0037898 ,\n",
       "          0.01163294,  0.01947608],\n",
       "        ...,\n",
       "        [ 0.15318275,  0.00484962,  0.15803237, ...,  0.00072157,\n",
       "          0.01640784,  0.03209412],\n",
       "        [ 0.1545851 ,  0.00506202,  0.15964712, ..., -0.00100078,\n",
       "          0.01468549,  0.03037176],\n",
       "        [ 0.1499451 ,  0.00502548,  0.15497057, ...,  0.0042149 ,\n",
       "          0.01205804,  0.01990118]],\n",
       "\n",
       "       [[-0.08593882,  0.00665472, -0.07928411, ...,  0.00121882,\n",
       "          0.0169051 ,  0.02474824],\n",
       "        [-0.09341647,  0.00532467, -0.0880918 , ...,  0.00247843,\n",
       "          0.01816471,  0.02600784],\n",
       "        [-0.09243608,  0.00586793, -0.08656815, ...,  0.00150588,\n",
       "          0.01719216,  0.03287843],\n",
       "        ...,\n",
       "        [-0.0405098 ,  0.00639352, -0.03411629, ...,  0.00155922,\n",
       "          0.01724549,  0.05646118],\n",
       "        [-0.05189333,  0.00776687, -0.04412646, ...,  0.00169725,\n",
       "          0.01738353,  0.02522667],\n",
       "        [-0.07166275,  0.00765991, -0.06400284, ...,  0.00713882,\n",
       "          0.01498196,  0.0228251 ]]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_uuid": "2f7342296138f6bfd3e9cedd029e1035de3b98fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 6s 872us/step\n",
      "6779/6779 [==============================] - 6s 885us/step\n",
      "6779/6779 [==============================] - 6s 873us/step\n",
      "6779/6779 [==============================] - 6s 886us/step\n"
     ]
    }
   ],
   "source": [
    "preds_test = []\n",
    "for i in range(4):\n",
    "    model.load_weights('weights_{}.h5'.format(i))\n",
    "    pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "    pred_3 = []\n",
    "    for pred_scalar in pred:\n",
    "        for i in range(3):\n",
    "            pred_3.append(pred_scalar)\n",
    "    preds_test.append(pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.asarray(preds_test[1])>0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test_3 = (np.asarray(preds_test[1])>0.51).astype(np.int).reshape(20337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_uuid": "9f76c471eaf983707d446c5081ab3d50c4e40ea5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20337,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test_2 = (np.squeeze(np.mean(preds_test, axis=0)) > 0.5).astype(np.int)\n",
    "preds_test_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "_uuid": "b35723f85d494b4b6ec630dd7c79135a110a4062"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_id  target\n",
       "0       8712       0\n",
       "1       8713       0\n",
       "2       8714       0\n",
       "3       8715       0\n",
       "4       8716       0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'] = preds_test_3\n",
    "submission.to_csv('../output/submission_27.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7600d0093a9880003240ef9ce0a1f1303e4d982"
   },
   "outputs": [],
   "source": [
    "lstm_preds = np.squeeze(np.mean(preds_test, axis=0))\n",
    "np.save(\"./lstm_preds.npy\", lstm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds_test_2[preds_test_2==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19404\n",
       "1      933\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6779/6779 [==============================] - 17s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights_0.h5')\n",
    "pred = model.predict(X_test, batch_size=300, verbose=1)\n",
    "pred_3 = []\n",
    "for pred_scalar in pred:\n",
    "    for i in range(3):\n",
    "        pred_3.append(pred_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5823f4244eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_3\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "pred_3>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_3 = (np.squeeze(pred_3) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds_3[preds_3==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = preds_3\n",
    "submission.to_csv('../output/submission_22.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19128\n",
       "1     1209\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
